# API LLM avec vLLM et Modal

Ce projet d√©ploie un mod√®le de langage Qwen3-8B-FP8 sur Modal avec une API compatible OpenAI utilisant vLLM.

## Table des mati√®res

- [Pr√©requis](#pr√©requis)
- [Installation](#installation)
- [Configuration](#configuration)
- [D√©ploiement](#d√©ploiement)
- [Utilisation](#utilisation)
- [Endpoints disponibles](#endpoints-disponibles)
- [Exemples de code](#exemples-de-code)
- [Gestion des co√ªts](#gestion-des-co√ªts)
- [Troubleshooting](#troubleshooting)

## Pr√©requis

- Python 3.9 ou sup√©rieur
- Un compte Modal (https://modal.com)
- Token Modal (Token ID et Token Secret)

## Installation

1. Cloner ou naviguer vers le r√©pertoire du projet :

```bash
cd /Users/lucasbometon/Desktop/api_llm/Modal
```

2. Cr√©er un environnement virtuel :

```bash
python3 -m venv venv
```

3. Activer l'environnement virtuel :

```bash
source venv/bin/activate
```

4. Installer Modal :

```bash
pip install modal
```

## Configuration

### Authentification Modal

Configurer votre token Modal avec vos identifiants :

```bash
modal token set --token-id <VOTRE_TOKEN_ID> --token-secret <VOTRE_TOKEN_SECRET>
```

### Configuration du mod√®le

Dans le fichier `vllm_inference.py`, vous pouvez modifier :

- **MODEL_NAME** : Le mod√®le Hugging Face √† utiliser (d√©faut: `Qwen/Qwen3-8B-FP8`)
- **N_GPU** : Nombre de GPUs H100 √† utiliser (d√©faut: 1)
- **FAST_BOOT** : Mode de d√©marrage rapide (True) ou performance optimale (False)
- **VLLM_PORT** : Port du serveur vLLM (d√©faut: 8000)

```python
MODEL_NAME = "Qwen/Qwen3-8B-FP8"
N_GPU = 1
FAST_BOOT = True
```

## D√©ploiement

### D√©ploiement sur Modal

Pour d√©ployer l'application sur Modal :

```bash
modal deploy vllm_inference.py
```

Cette commande va :
1. Construire l'image Docker avec CUDA et vLLM
2. T√©l√©charger les poids du mod√®le
3. D√©ployer l'application
4. Retourner une URL publique

### Test du d√©ploiement

Pour tester localement le d√©ploiement :

```bash
modal run vllm_inference.py
```

## Utilisation

### URL de l'API

Apr√®s le d√©ploiement, votre API est accessible √† :

```
https://lbometon2--example-vllm-inference-serve.modal.run
```

### Documentation interactive

Swagger UI disponible √† :

```
https://lbometon2--example-vllm-inference-serve.modal.run/docs
```

## Endpoints disponibles

### Health Check

```bash
GET /health
```

V√©rifie que le serveur r√©pond correctement.

```bash
curl https://lbometon2--example-vllm-inference-serve.modal.run/health
```

### Chat Completions

```bash
POST /v1/chat/completions
```

Endpoint principal pour les conversations avec le mod√®le.

**Param√®tres** :
- `model` : Nom du mod√®le (utilisez "llm")
- `messages` : Liste des messages au format OpenAI
- `stream` : Boolean pour activer le streaming (optionnel)
- `temperature` : Temp√©rature de g√©n√©ration (optionnel, 0.0-2.0)
- `max_tokens` : Nombre maximum de tokens √† g√©n√©rer (optionnel)

## Exemples de code

### Avec curl

```bash
curl -X POST "https://lbometon2--example-vllm-inference-serve.modal.run/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llm",
    "messages": [
      {"role": "system", "content": "Tu es un assistant utile."},
      {"role": "user", "content": "Explique-moi la d√©composition en valeurs singuli√®res."}
    ],
    "stream": false
  }'
```

### Avec Python et OpenAI SDK

Installer la biblioth√®que OpenAI :

```bash
pip install openai
```

Code Python :

```python
from openai import OpenAI

# Initialiser le client
client = OpenAI(
    base_url="https://lbometon2--example-vllm-inference-serve.modal.run/v1",
    api_key="not-needed"  # L'API ne requiert pas de cl√©
)

# Envoyer une requ√™te
response = client.chat.completions.create(
    model="llm",
    messages=[
        {"role": "system", "content": "Tu es un assistant utile."},
        {"role": "user", "content": "Bonjour! Comment √ßa va?"}
    ]
)

print(response.choices[0].message.content)
```

### Streaming avec Python

```python
from openai import OpenAI

client = OpenAI(
    base_url="https://lbometon2--example-vllm-inference-serve.modal.run/v1",
    api_key="not-needed"
)

# Streaming activ√©
stream = client.chat.completions.create(
    model="llm",
    messages=[
        {"role": "user", "content": "√âcris-moi un po√®me sur l'intelligence artificielle."}
    ],
    stream=True
)

# Afficher les chunks au fur et √† mesure
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

### Avec aiohttp (async)

```python
import aiohttp
import json

async def query_llm():
    url = "https://lbometon2--example-vllm-inference-serve.modal.run/v1/chat/completions"

    payload = {
        "model": "llm",
        "messages": [
            {"role": "user", "content": "Quelle est la capitale de la France?"}
        ],
        "stream": False
    }

    async with aiohttp.ClientSession() as session:
        async with session.post(url, json=payload) as resp:
            result = await resp.json()
            print(result['choices'][0]['message']['content'])

# Ex√©cuter
import asyncio
asyncio.run(query_llm())
```

### Avec JavaScript/Node.js

```javascript
const OpenAI = require('openai');

const client = new OpenAI({
  baseURL: 'https://lbometon2--example-vllm-inference-serve.modal.run/v1',
  apiKey: 'not-needed'
});

async function main() {
  const completion = await client.chat.completions.create({
    model: 'llm',
    messages: [
      { role: 'user', content: 'Bonjour!' }
    ]
  });

  console.log(completion.choices[0].message.content);
}

main();
```

## Gestion des co√ªts

### Mise en veille automatique

Le serveur se met automatiquement en veille apr√®s **15 minutes** d'inactivit√© pour r√©duire les co√ªts.

Configuration dans le code :

```python
scaledown_window=15 * MINUTES
```

### Cold Start

Lors du premier appel apr√®s une p√©riode d'inactivit√©, le d√©marrage peut prendre quelques minutes :
- Avec `FAST_BOOT=True` : ~30-60 secondes
- Avec `FAST_BOOT=False` : ~2-3 minutes (mais meilleures performances ensuite)

### Volumes Modal

Deux volumes sont utilis√©s pour le cache :
- `huggingface-cache` : Cache des poids du mod√®le
- `vllm-cache` : Cache des artefacts de compilation vLLM

Ces volumes persistent entre les d√©ploiements pour acc√©l√©rer les d√©marrages.

## Troubleshooting

### Erreur : "modal: command not found"

Assurez-vous que l'environnement virtuel est activ√© :

```bash
source venv/bin/activate
```

Ou utilisez le chemin complet :

```bash
./venv/bin/modal deploy vllm_inference.py
```

### Timeout lors du d√©ploiement

Le premier d√©ploiement peut prendre 5-10 minutes pour t√©l√©charger tous les composants. Augmentez le timeout si n√©cessaire :

```python
timeout=10 * MINUTES
```

### Erreur 503 ou 504

Le serveur est probablement en train de d√©marrer (cold start). Attendez 1-2 minutes et r√©essayez.

### Changer de mod√®le

Pour utiliser un autre mod√®le, modifiez dans `vllm_inference.py` :

```python
MODEL_NAME = "meta-llama/Llama-3-8b"  # Exemple
MODEL_REVISION = "main"  # ou un commit sp√©cifique
```

V√©rifiez que le mod√®le est compatible avec vLLM et qu'il tient dans la m√©moire GPU (8GB pour un H100).

## Monitoring

### Dashboard Modal

Consultez les logs et m√©triques sur le dashboard Modal :

```
https://modal.com/apps/lbometon2/main/deployed/example-vllm-inference
```

### Logs en temps r√©el

```bash
modal logs example-vllm-inference
```

## Support

- Documentation Modal : https://modal.com/docs
- Documentation vLLM : https://docs.vllm.ai
- Issues GitHub Modal : https://github.com/modal-labs/modal-examples

## Structure du projet

```
Modal/
‚îú‚îÄ‚îÄ venv/                  # Environnement virtuel Python
‚îú‚îÄ‚îÄ vllm_inference.py      # Script principal de d√©ploiement
‚îî‚îÄ‚îÄ README.md              # Cette documentation
```

## Licence

Ce projet utilise des technologies open-source. Consultez les licences respectives de Modal, vLLM et Qwen pour plus d'informations.


‚úÖ Tests r√©ussis ! Voici les r√©sultats
Test 1 : Sans param√®tres anti-r√©p√©tition
{
  "max_tokens": 50
}
R√©sultat : Le thinking a d√©marr√© mais s'est arr√™t√© trop t√¥t (limite de tokens)
Pas de r√©p√©tition visible, mais texte tronqu√©
Test 2 : Avec param√®tres anti-r√©p√©tition (mod√©r√©s)
{
  "temperature": 0.7,
  "frequency_penalty": 0.5,
  "top_p": 0.9,
  "max_tokens": 200,
  "stop": ["</think>"]
}
‚úÖ Le stop: ["</think>"] a correctement arr√™t√© apr√®s le thinking
‚úÖ Texte fluide, pas de r√©p√©tition
‚úÖ 139 tokens g√©n√©r√©s
Test 3 : Avec param√®tres anti-r√©p√©tition √©lev√©s
{
  "temperature": 0.7,
  "frequency_penalty": 1.0,  // P√©nalit√© √©lev√©e
  "top_p": 0.9,
  "max_tokens": 300,
  "stop": ["</think>"]
}
‚úÖ Aucune r√©p√©tition m√™me avec 300 tokens de thinking
‚úÖ Vocabulaire tr√®s vari√© : "cardiovascular", "endorphins", "weight management", "bone density", "osteoporosis", "metabolism", "stamina"
‚úÖ Le mod√®le raisonne de fa√ßon coh√©rente sans boucler
üéØ Conclusion
Votre API vLLM sur Modal fonctionne parfaitement !
Param√®tres recommand√©s pour √©viter les r√©p√©titions :
{
    "temperature": 0.7,          # Bon √©quilibre cr√©ativit√©/coh√©rence
    "frequency_penalty": 0.5,    # Usage g√©n√©ral
    "frequency_penalty": 1.0,    # Si r√©p√©titions s√©v√®res
    "top_p": 0.9,
    "max_tokens": 300-1000,
    "stop": ["</think>", "\n\n\n"]
}
Fichiers de r√©f√©rence :
üìñ README.md - Documentation compl√®te avec exemples
üìö ANTI_REPETITION_GUIDE.md - Guide d√©taill√© des param√®tres
üß™ client_example.py - Scripts Python pour tester
Les boucles "the the the..." sont maintenant √©vit√©es gr√¢ce √† frequency_penalty ! üöÄ