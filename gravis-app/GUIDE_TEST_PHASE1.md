# Guide de Test - Phase 1 RAG Improvements üß™

## üöÄ Comment tester les am√©liorations

### √âtape 1 : Rebuild l'application

```bash
cd /Users/lucasbometon/Desktop/voice_flow/gravis/gravis-app

# Rebuild Tauri
npm run tauri build

# OU en mode dev
npm run tauri dev
```

---

### √âtape 2 : Nettoyer la base RAG existante

‚ö†Ô∏è **IMPORTANT** : Les anciens documents utilisent le chunking 1024 tokens (obsol√®te).

**Dans l'interface Gravis** :
1. Aller dans l'onglet RAG
2. Lister tous les documents : `list_rag_documents`
3. Supprimer TOUS les documents un par un
4. V√©rifier que Qdrant est vide

**Alternative via CLI** :
```bash
# Si vous avez acc√®s direct √† Qdrant
curl -X DELETE http://localhost:6333/collections/collection_default_group
```

---

### √âtape 3 : R√©injecter un document de test

**Document recommand√©** : `2510.18234v1.pdf` (DeepSeek OCR)

**Proc√©dure** :
1. Extraire le PDF via OCR (comme d'habitude)
2. Injecter le document dans le RAG
3. **Observer** : Nombre de chunks cr√©√©s devrait √™tre ~3x plus √©lev√© qu'avant

**Avant Phase 1** :
```
‚úÖ Document inject√© : 8 chunks cr√©√©s
```

**Apr√®s Phase 1** (attendu) :
```
‚úÖ Document inject√© : 24-30 chunks cr√©√©s
```

---

### √âtape 4 : Tester une requ√™te

**Requ√™te de test** :
```
"explique moi deepseek OCR"
```

#### R√©sultats AVANT Phase 1 :
```
üìä 5 r√©sultats trouv√©s

[Source 1] Score: 70.3%
Fichier: 2510.18234v1.pdf
Cat√©gorie: Mixed
Contenu: Abstract We present DeepSeek-OCR as an initial...

[Source 2] Score: 70.3%
Fichier: 2510.18234v1.pdf
Cat√©gorie: Mixed
Contenu: Abstract We present DeepSeek-OCR as an initial...

[Source 3] Score: 70.2%
Fichier: 2510.18234v1.pdf
Cat√©gorie: Mixed
Contenu: Abstract We present DeepSeek-OCR as an initial...

‚ùå PROBL√àME : Toutes les sources sont identiques !
```

#### R√©sultats APR√àS Phase 1 (attendu) :
```
üìä 5 r√©sultats trouv√©s

[Source 1] Score: 87.5%
Fichier: 2510.18234v1.pdf
Cat√©gorie: Mixed
Contenu: DeepSeek-OCR consists of two components: DeepEncoder...

[Source 2] Score: 82.1%
Fichier: 2510.18234v1.pdf
Cat√©gorie: Mixed
Contenu: The architecture includes a DeepEncoder for vision...

[Source 3] Score: 76.8%
Fichier: 2510.18234v1.pdf
Cat√©gorie: Mixed
Contenu: DeepSeek-OCR achieves 20x compression ratio while...

[Source 4] Score: 71.2%
Fichier: 2510.18234v1.pdf
Cat√©gorie: Mixed
Contenu: The model processes 200,000+ pages per day...

[Source 5] Score: 68.5%
Fichier: 2510.18234v1.pdf
Cat√©gorie: Mixed
Contenu: Evaluation shows 95% accuracy on text recognition...

‚úÖ R√âSULTAT : Sources DIVERSIFI√âES et COMPL√âMENTAIRES !
```

---

### √âtape 5 : V√©rifier la r√©ponse du LLM

**AVANT Phase 1** :
```
üß† R√©flexion du mod√®le

DeepSeek OCR est un syst√®me qui... [r√©p√®te 5x la m√™me info de l'abstract]

üìö Sources RAG (5 chunks en 196ms)
Source 1: 70.3% - Abstract We present...
Source 2: 70.3% - Abstract We present...
Source 3: 70.2% - Abstract We present...
...
```

**APR√àS Phase 1 (attendu)** :
```
üß† R√©flexion du mod√®le

DeepSeek OCR est un syst√®me de reconnaissance optique de caract√®res innovant [Source 1].

## Architecture
Le syst√®me utilise deux composants principaux [Source 2]:
- **DeepEncoder** : Encoder vision qui compresse les images en tokens
- **DeepSeek-3B-MoE** : D√©codeur de langage avec architecture MoE

## Performance
DeepSeek-OCR atteint un ratio de compression de 20:1 tout en maintenant
une pr√©cision de 95% [Source 3]. Le syst√®me peut traiter plus de 200 000
pages par jour sur un seul GPU H100 [Source 4].

## Cas d'usage
Applications incluent la num√©risation de documents, l'extraction de donn√©es
structur√©es, et l'indexation de corpus massifs [Source 5].

üìö Sources RAG (5 chunks en 196ms)
Source 1: 87.5% - DeepSeek-OCR consists of...
Source 2: 82.1% - The architecture includes...
Source 3: 76.8% - achieves 20x compression...
...

‚úÖ R√âSULTAT : R√©ponse SYNTH√âTIQUE avec infos COMPL√âMENTAIRES !
```

---

## üìä M√©triques √† valider

### 1. Chunking
| M√©trique | Avant | Apr√®s | Status |
|----------|-------|-------|--------|
| Chunks/doc | ~8-12 | ~24-36 | ‚¨ú √Ä v√©rifier |
| Taille chunk | ~1024 tokens | ~384 tokens | ‚úÖ Confirm√© |
| Overlap | 128 tokens | 48 tokens | ‚úÖ Confirm√© |

### 2. Diversit√© des scores
| M√©trique | Avant | Apr√®s | Status |
|----------|-------|-------|--------|
| Score source 1 | 70.3% | 85-90% | ‚¨ú √Ä v√©rifier |
| Score source 5 | 70.0% | 65-75% | ‚¨ú √Ä v√©rifier |
| √âcart min/max | <1% | 15-20% | ‚¨ú √Ä v√©rifier |

### 3. Qualit√© des r√©ponses
| Crit√®re | Avant | Apr√®s | Status |
|---------|-------|-------|--------|
| Redondance | Tr√®s haute | Basse | ‚¨ú √Ä v√©rifier |
| Compl√©tude | Moyenne | √âlev√©e | ‚¨ú √Ä v√©rifier |
| Citations uniques | 1 source | 3-5 sources | ‚¨ú √Ä v√©rifier |

---

## üß™ Tests avanc√©s

### Test 1 : V√©rifier les pr√©fixes E5

**Backend logs** √† surveiller :
```rust
// Lors de l'indexation
üßÆ Generating embeddings for 24 chunks
[INFO] Encoding with prefix: "passage: <content>"

// Lors de la recherche
üîç Searching RAG for: "explique moi deepseek"
[INFO] Encoding with prefix: "query: explique moi deepseek"
```

### Test 2 : V√©rifier la normalisation L2

**Code de test** :
```rust
#[tokio::test]
async fn test_embedding_normalized() {
    let embedder = CustomE5Embedder::new(CustomE5Config::default()).await.unwrap();
    let emb = embedder.encode("test").await.unwrap();

    let norm: f32 = emb.iter().map(|x| x*x).sum::<f32>().sqrt();

    println!("Norme L2: {}", norm);
    assert!((norm - 1.0).abs() < 1e-4, "Embedding non normalis√© !");
}
```

### Test 3 : Benchmark avant/apr√®s

```bash
# Cr√©er un script de benchmark
cat > benchmark_rag.sh << 'EOF'
#!/bin/bash

echo "=== BENCHMARK RAG PHASE 1 ==="
echo ""

# 1. Injection
echo "1. Injection document..."
START=$(date +%s%3N)
# Votre commande d'injection ici
END=$(date +%s%3N)
INJECTION_TIME=$((END - START))
echo "   Temps: ${INJECTION_TIME}ms"

# 2. Recherche
echo "2. Recherche RAG..."
START=$(date +%s%3N)
# Votre commande de recherche ici
END=$(date +%s%3N)
SEARCH_TIME=$((END - START))
echo "   Temps: ${SEARCH_TIME}ms"

# 3. R√©sum√©
echo ""
echo "=== R√âSUM√â ==="
echo "Injection: ${INJECTION_TIME}ms"
echo "Recherche: ${SEARCH_TIME}ms"
EOF

chmod +x benchmark_rag.sh
./benchmark_rag.sh
```

---

## ‚ùì Troubleshooting

### Probl√®me 1 : Toujours des r√©sultats redondants

**Cause probable** : Documents anciens pas supprim√©s

**Solution** :
```bash
# Supprimer compl√®tement la collection Qdrant
curl -X DELETE http://localhost:6333/collections/collection_default_group

# V√©rifier qu'elle est supprim√©e
curl http://localhost:6333/collections

# R√©injecter les documents
```

### Probl√®me 2 : Moins de chunks que pr√©vu

**Cause probable** : Document trop court ou chunking pas appliqu√©

**V√©rification** :
```rust
// Dans les logs, chercher :
üìä Smart chunking created X chunks (avg: Y chars, detected Z sections)

// X devrait √™tre ~3x plus √©lev√© qu'avant
```

### Probl√®me 3 : Scores tous identiques

**Cause probable** : Embeddings pas re-g√©n√©r√©s

**Solution** :
1. V√©rifier que `encode_document` est bien appel√© (logs)
2. V√©rifier le cache embeddings (peut-√™tre vider)
3. Restart l'application Tauri

### Probl√®me 4 : LLM r√©p√®te toujours les infos

**Cause probable** : Prompt syst√®me pas appliqu√©

**V√©rification** :
```bash
# Dans les logs Tauri, chercher :
‚úÖ RAG context prepared: X chunks, Y sources, Zms

# Le contexte doit contenir le nouveau prompt avec:
"**INSTRUCTIONS POUR R√âPONDRE**:"
"1. **Analyse et synth√®se**: ..."
```

---

## üìù Checklist finale

Avant de valider la Phase 1 :

- [ ] Code compile sans erreurs (`cargo check`)
- [ ] Tests unitaires passent (`cargo test`)
- [ ] Application se lance (`npm run tauri dev`)
- [ ] Documents anciens supprim√©s
- [ ] Nouveau document inject√© avec ~3x plus de chunks
- [ ] Recherche retourne des scores diversifi√©s (65-90%)
- [ ] R√©ponse LLM synth√©tique et non redondante
- [ ] Logs montrent les pr√©fixes E5 (`query:` et `passage:`)
- [ ] Normalisation L2 active (norme ‚âà 1.0)

---

## üéØ R√©sultat attendu

**Si tout fonctionne correctement** :

‚úÖ Chunks plus petits et coh√©rents
‚úÖ Scores diversifi√©s (pas tous √† 70%)
‚úÖ R√©ponses synth√©tiques sans redondance
‚úÖ Citations de sources vari√©es
‚úÖ Performance globale am√©lior√©e de 2-3x

**Temps de validation estim√©** : 15-30 minutes

---

## üîú Prochaines √©tapes

Si Phase 1 valid√©e avec succ√®s :

1. **Phase 2A** : Impl√©menter MMR re-ranking (3h)
2. **Phase 2B** : Ajouter cross-encoder (1 jour)
3. **Phase 3** : Hybrid search BM25 + Vector (2 jours)

---

**Date** : 2025-11-07
**Version** : Phase 1 - Guide de Test
**Auteur** : Claude Code
