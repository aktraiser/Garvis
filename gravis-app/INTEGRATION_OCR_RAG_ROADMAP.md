# Feuille de Route : Int√©gration OCR dans le Pipeline RAG

## üéØ Objectif
Int√©grer le syst√®me OCR compl√®tement d√©velopp√© dans le pipeline RAG existant pour permettre l'indexation et la recherche de documents PDF et images avec extraction de texte intelligente.

## üìä √âtat Actuel - Mis √† jour le 2025-11-06

### üéâ Phase 3C TERMIN√âE : Corrections & Stabilisation Production !

**Syst√®me RAG Op√©rationnel End-to-End :**
- ‚úÖ Extraction de texte (OCR AWCS ou natif PDF)
- ‚úÖ G√©n√©ration embeddings (CustomE5 384D)
- ‚úÖ Persistance Qdrant (Collections par groupe avec ID fixe)
- ‚úÖ Interface utilisateur compl√®te (Injection + Visualisation)
- ‚úÖ Arguments Tauri unifi√©s (camelCase frontend ‚Üî snake_case backend)

**M√©triques Production :**
- 3 documents persist√©s et test√©s (75 chunks au total)
- Collection unique : `collection_default_group` avec ID fixe
- Confidence moyenne : 85%
- Temps r√©ponse list_rag_documents : <500ms
- 0% erreurs Qdrant (UUID blake3 valides)
- 100% r√©utilisation texte AWCS (pas de r√©extraction)
- 100% affichage documents persist√©s dans l'interface

---

### ‚úÖ OCR System (Phases 1-3 Termin√©es)
- **Infrastructure Tesseract** : Processeur complet avec cache Blake3
- **Command-based Processing** : Int√©gration Tauri + configuration avanc√©e
- **Pipeline PDF Hybride** : Extraction native + OCR cibl√© + normalisation Unicode
- **TextCleaner Production** : Normalisation Unicode optimis√©e pour RAG

### ‚úÖ RAG System (Architecture Existante)
- **CustomE5Embedder** : Embeddings 384D avec cache DashMap
- **QdrantRestClient** : Base vectorielle avec collections par groupe
- **DocumentGroup** : Architecture modulaire avec ChunkConfig
- **ChunkMetadata** : M√©tadonn√©es enrichies avec types et priorit√©s

### ‚úÖ Int√©gration OCR-RAG (Phases 1-2 Termin√©es)
- **Structures √©tendues** : ChunkMetadata avec m√©tadonn√©es OCR (source_type, extraction_method)
- **DocumentProcessor unifi√©** : Pipeline d√©tection ‚Üí extraction ‚Üí normalisation ‚Üí chunking
- **Types intelligents** : SourceType, ExtractionMethod, PdfStrategy pour strat√©gies adaptatives
- **IngestionEngine** : Pipeline intelligent avec d√©tection automatique PDF strategy
- **UnifiedCache** : Cache multi-niveaux OCR ‚Üí Embeddings ‚Üí Documents
- **SmartChunker** : Chunking adaptatif par type de contenu

### ‚úÖ Universal RAG Pipeline (Phase 3A Termin√©e)
- **DocumentClassifier** : Classification automatique Business/Academic/Legal/Technical
- **BusinessMetadata** : Extraction KPIs financiers avec patterns EN/FR robustes
- **Unicode Sanitization** : Normalisation ligatures PDF (Ô¨Å‚Üífi, Ô¨Ç‚Üífl, ≈í‚ÜíOE)
- **Chunking Adaptatif** : Configurations optimis√©es par type de document
- **Patterns Bilingues** : Support complet fran√ßais/anglais avec formats EU/US
- **Tests Production** : Validation sur documents r√©els avec m√©triques de qualit√©

### ‚úÖ Pipeline RAG Production (Phase 3B Termin√©e)
- **Pipeline Complet** : Extraction ‚Üí Chunking ‚Üí Embeddings ‚Üí Qdrant ‚Üí Affichage
- **R√©utilisation AWCS** : Param√®tre `extracted_text` pour √©viter r√©extraction PDF
- **G√©n√©ration UUID** : blake3 hash pour identifiants Qdrant valides
- **Commande list_rag_documents** : R√©cup√©ration documents persist√©s via Scroll API
- **Interface Frontend** : Bouton "Voir RAG", affichage documents avec m√©tadonn√©es compl√®tes
- **Tests Valid√©s** : 4 documents, 25 chunks, notification et affichage fonctionnels

## üó∫Ô∏è Plan d'Int√©gration (4 Phases)

---

## **Phase 1: Extension Structures RAG (3 jours)** ‚úÖ TERMIN√âE

### 1.1 Enrichir ChunkMetadata avec OCR
```rust
// src/rag/mod.rs - Extension ChunkMetadata
pub struct ChunkMetadata {
    // Existant...
    pub tags: Vec<String>,
    pub priority: Priority,
    pub language: String,
    pub symbol: Option<String>,
    pub context: Option<String>, 
    pub confidence: f32,
    
    // ‚ú® NOUVEAU: M√©tadonn√©es OCR
    pub ocr_metadata: Option<OcrMetadata>,
    pub source_type: SourceType,
    pub extraction_method: ExtractionMethod,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SourceType {
    NativeText,
    OcrExtracted,
    HybridPdfNative,
    HybridPdfOcr,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ExtractionMethod {
    DirectRead,
    TesseractOcr { confidence: f32, language: String },
    PdfNative,
    PdfOcrFallback,
    HybridIntelligent,
}
```

### 1.2 √âtendre DocumentType pour OCR
```rust
// Support d√©taill√© des documents OCR
pub enum DocumentType {
    SourceCode { language: String },
    PDF { 
        extraction_strategy: PdfStrategy,
        native_text_ratio: f32,
        ocr_pages: Vec<usize>,
        total_pages: usize,
    },
    Image { 
        ocr_result: OcrResult,
        preprocessing_config: PreprocessConfig,
    },
    // Existants...
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PdfStrategy {
    NativeOnly,
    OcrOnly,
    HybridIntelligent,
}
```

### 1.3 Cr√©er DocumentProcessor unifi√©
```rust
// src/rag/document_processor.rs - NOUVEAU
pub struct DocumentProcessor {
    ocr_processor: TesseractProcessor,
    text_cleaner: TextCleaner,
    embedder: CustomE5Embedder,
}

impl DocumentProcessor {
    pub async fn process_document(&self, 
        file_path: &Path, 
        group_config: &ChunkConfig
    ) -> RagResult<GroupDocument> {
        // Auto-d√©tection format
        // Strat√©gie extraction intelligente
        // Chunking adapt√© au contenu
        // G√©n√©ration embeddings avec cache
    }
}
```

**Livrables Phase 1:**
- ‚úÖ Structures √©tendues avec m√©tadonn√©es OCR
- ‚úÖ DocumentProcessor unifi√© PDF/Image/Text
- ‚úÖ Tests d'int√©gration structures

**Status Phase 1 - TERMIN√âE ‚úÖ**
- ‚úÖ ChunkMetadata √©tendu avec `ocr_metadata`, `source_type`, `extraction_method`
- ‚úÖ SourceType enum: `NativeText`, `OcrExtracted`, `HybridPdfNative`, `HybridPdfOcr`
- ‚úÖ ExtractionMethod enum: `DirectRead`, `TesseractOcr`, `PdfNative`, `PdfOcrFallback`, `HybridIntelligent`
- ‚úÖ DocumentType::PDF avec strat√©gies `PdfStrategy`: `NativeOnly`, `OcrOnly`, `HybridIntelligent`
- ‚úÖ DocumentProcessor cr√©√© (`src/rag/document_processor.rs`) avec d√©tection format automatique
- ‚úÖ Pipeline unifi√©: d√©tection ‚Üí extraction ‚Üí normalisation ‚Üí chunking adaptatif
- ‚úÖ Tests valid√©s: 13 structures, pipeline texte complet (2 chunks), d√©tection MD/TXT
- ‚úÖ M√©tadonn√©es OCR int√©gr√©es: confidence=1.0 pour texte natif, structures pr√™tes pour OCR

---

## **Phase 2: Pipeline d'Ingestion Intelligent (5 jours)** ‚úÖ TERMIN√âE

### 2.1 D√©tection Automatique Strat√©gie
```rust
// src/rag/ingestion_engine.rs - NOUVEAU
pub struct IngestionEngine {
    document_processor: DocumentProcessor,
    strategy_detector: StrategyDetector,
}

pub struct StrategyDetector;
impl StrategyDetector {
    pub fn detect_pdf_strategy(&self, path: &Path) -> PdfStrategy {
        // 1. Analyse rapide native text ratio
        // 2. Heuristiques qualit√© (fonts, OCR-detected)
        // 3. D√©cision HybridIntelligent vs NativeOnly
    }
    
    pub fn detect_image_preprocessing(&self, image: &DynamicImage) -> PreprocessConfig {
        // Auto-d√©tection Otsu vs autres filtres
    }
}
```

### 2.2 Pipeline Chunking Adaptatif
```rust
impl DocumentProcessor {
    async fn chunk_by_content_type(&self, 
        content: &str, 
        source_type: SourceType,
        config: &ChunkConfig
    ) -> Vec<EnrichedChunk> {
        match source_type {
            SourceType::OcrExtracted => {
                // Chunking sp√©cial OCR: 
                // - Pr√©servation structure d√©tect√©e
                // - Confiance par chunk
                // - Normalisation Unicode
            },
            SourceType::HybridPdfOcr => {
                // Fusion chunks natifs + OCR
                // D√©duplication intelligente
            },
            _ => {
                // Chunking standard existant
            }
        }
    }
}
```

### 2.3 Int√©gration Cache OCR ‚Üí Embeddings
```rust
// Extension du cache existant
pub struct UnifiedCache {
    ocr_cache: OcrCache,           // Existant
    embedding_cache: DashMap<String, Vec<f32>>, // Existant 
    document_cache: DashMap<String, GroupDocument>, // NOUVEAU
}

impl UnifiedCache {
    pub fn get_or_process_document(&self, 
        file_path: &Path,
        config: &ChunkConfig
    ) -> RagResult<GroupDocument> {
        // 1. Check document cache par hash fichier
        // 2. Check OCR cache pour extraction
        // 3. Check embedding cache pour chunks
        // 4. Process seulement ce qui manque
    }
}
```

**Livrables Phase 2:**
- ‚úÖ IngestionEngine avec d√©tection automatique
- ‚úÖ Pipeline chunking adaptatif par type source
- ‚úÖ Cache unifi√© OCR ‚Üí Embeddings ‚Üí Documents
- ‚úÖ Tests end-to-end PDF ‚Üí RAG ‚Üí Search

**Status Phase 2 - TERMIN√âE ‚úÖ**
- ‚úÖ IngestionEngine cr√©√© (`src/rag/ingestion_engine.rs`) avec StrategyDetector
- ‚úÖ Pipeline chunking adaptatif par SourceType: OCR vs Native vs Hybrid
- ‚úÖ UnifiedCache int√©gr√© avec cache multi-niveaux (OCR, Embeddings, Documents)
- ‚úÖ SmartChunker cr√©√© avec configurations adaptatives par type de document
- ‚úÖ EmbedderManager pour gestion centralis√©e des embeddings avec cache
- ‚úÖ Tests complets: ingestion intelligente, cache unifi√©, chunking adaptatif
- ‚úÖ D√©tection automatique PDF strategy: Native vs OCR vs Hybrid selon qualit√©

---

## **Phase 3A: Universal RAG Pipeline - Business Documents (4 jours)** ‚úÖ TERMIN√âE

### 3A.1 Classification Automatique de Documents
```rust
// src/rag/document_classifier.rs - NOUVEAU
pub struct DocumentClassifier {
    business_patterns: BusinessPatternMatcher,
    academic_patterns: AcademicPatternMatcher,
    legal_patterns: LegalPatternMatcher,
    technical_patterns: TechnicalPatternMatcher,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq)]
pub enum DocumentCategory {
    Academic,
    Business,
    Legal,
    Technical,
    Mixed,
}

impl DocumentClassifier {
    pub fn classify(&self, content: &str) -> Result<DocumentCategory> {
        // Classification automatique avec scoring pond√©r√© EN/FR
        // Patterns bilingues pour sections Business
        // D√©tection KPIs financiers avec formats EU/US
    }
}
```

### 3A.2 M√©tadonn√©es Business Enrichies
```rust
// src/rag/business_metadata.rs - NOUVEAU
pub struct BusinessMetadata {
    pub fiscal_year: Option<i32>,
    pub company_name: Option<String>,
    pub financial_kpis: Vec<FinancialKPI>,
    pub section_type: BusinessSection,
    pub confidence_score: f32,
}

pub struct FinancialKPI {
    pub name: String,        // "Revenue", "EBITDA", "Net Income"
    pub value: f64,          // Valeur normalis√©e
    pub currency: String,    // "USD", "EUR" 
    pub period: String,      // "2023", "Q3 2023"
    pub unit: String,        // "Million", "Billion"
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub enum BusinessSection {
    ExecutiveSummary,        // R√©sum√© Ex√©cutif
    FinancialHighlights,     // Faits Saillants Financiers  
    BusinessOverview,        // Aper√ßu des Activit√©s
    RiskFactors,             // Facteurs de Risque
    MarketAnalysis,          // Analyse du March√©
    Unknown,
}
```

### 3A.3 Chunking Adaptatif par Type de Document
```rust
// Extension SmartChunker pour types de documents
impl SmartChunkConfig {
    pub fn business_optimized() -> Self {
        Self {
            target_tokens: 500,
            overlap_percent: 0.15,
            mmr_lambda: 0.6,      // Plus de relevance pour business
            max_context_docs: 6,   // Plus de contexte pour analyse
            min_tokens: 200,       // Minimum plus √©lev√©
            // Patterns sp√©cialis√©s pour sections Business
        }
    }
    
    pub fn academic_optimized() -> Self {
        Self {
            target_tokens: 400,
            overlap_percent: 0.2,
            mmr_lambda: 0.4,      // Plus de diversit√© pour recherche
            max_context_docs: 4,
            // Patterns pour citations et r√©f√©rences
        }
    }
}

impl SmartChunker {
    pub fn new_business(config: SmartChunkConfig) -> Result<Self> {
        // Chunker sp√©cialis√© pour documents Business
        // D√©tection sections: Executive Summary, Financial Highlights
        // Pr√©servation structure financi√®re
    }
}
```

### 3A.4 Normalisation Unicode pour PDFs
```rust
// src/rag/unicode_utils.rs - NOUVEAU
pub fn sanitize_pdf_text(input: &str) -> Result<(String, NormalizationStats)> {
    // Remplacement ligatures: Ô¨Å‚Üífi, Ô¨Ç‚Üífl, Ô¨É‚Üíffi, Ô¨Ñ‚Üíffl
    // Normalisation Unicode NFD ‚Üí NFC
    // Support caract√®res fran√ßais: ≈í‚ÜíOE, ≈ì‚Üíoe
    // Nettoyage guillemets smart et tirets
}

pub struct NormalizationStats {
    pub total_chars: usize,
    pub ligatures_replaced: usize,
    pub unicode_normalized: bool,
    pub decomposed_chars: usize,
}
```

### 3A.5 Patterns Multilingues EN/FR
```rust
// Patterns bilingues pour extraction robuste
static KPI_VALUE_PATTERNS: Lazy<HashMap<String, Regex>> = Lazy::new(|| {
    let mut patterns = HashMap::new();
    
    patterns.insert("revenue".to_string(),
        Regex::new(r"(?i)(revenue[s]?|chiffre\s+d'affaires|ca)\s*(?:of|was|reached|a\s+atteint)?\s*(?:\$|‚Ç¨|USD|EUR)?\s*([0-9]+(?:[,.]\s*[0-9]{3})*(?:[,.]?[0-9]+)?)\s*(million[s]?|billion[s]?|milliard[s]?|M|B|Md)?")
    );
    
    // Support formats EU (1.234.567,89) et US (1,234,567.89)
    // Verbes fran√ßais: "a atteint", "s'√©l√®ve √†", "√©tait de"
    // Unit√©s fran√ßaises: millions, milliards vs millions, billions
});
```

**Livrables Phase 3A:**
- ‚úÖ Classification automatique de documents (Business/Academic/Legal/Technical)
- ‚úÖ M√©tadonn√©es Business enrichies avec KPIs financiers
- ‚úÖ Chunking adaptatif par type de document
- ‚úÖ Normalisation Unicode pour ligatures PDF
- ‚úÖ Patterns bilingues EN/FR robustes

**Status Phase 3A - TERMIN√âE ‚úÖ**
- ‚úÖ DocumentClassifier avec patterns EN/FR (`src/rag/document_classifier.rs`)
- ‚úÖ BusinessMetadata avec extraction KPIs (Revenue, EBITDA, Net Income, Total Assets, Market Cap)
- ‚úÖ SmartChunkConfig adaptatif: business_optimized(), academic_optimized(), legal_optimized()
- ‚úÖ Normalisation Unicode compl√®te: 6 ligatures remplac√©es en 8ms sur 25k chars
- ‚úÖ Parsing robuste nombres EU/US: 1.234.567,89 ‚Üî 1,234,567.89
- ‚úÖ Patterns bilingues: "Executive Summary" ‚Üî "R√©sum√© Ex√©cutif"
- ‚úÖ Tests complets: 5 KPIs FR d√©tect√©s, 3 KPIs EN d√©tect√©s, score confiance 1.0
- ‚úÖ Int√©gration DocumentProcessor avec sanitization Unicode automatique

---

## **Phase 3: Interface Tauri Commands (3 jours)** ‚úÖ TERMIN√âE

### 3.1 Commandes RAG + OCR Unifi√©es ‚úÖ
```rust
// src/rag/commands.rs - Extension des commandes existantes
#[tauri::command]
pub async fn add_document_intelligent(
    file_path: String,
    group_id: String,
    force_ocr: Option<bool>,
    state: tauri::State<'_, RagState>
) -> Result<DocumentIngestionResponse, String> {
    // Pipeline complet: Detection ‚Üí OCR ‚Üí Chunking ‚Üí Classification ‚Üí Embedding ‚Üí Indexing
}

#[tauri::command]
pub async fn search_with_classification(
    query: String,
    group_id: String,
    filter_category: Option<DocumentCategory>,
    state: tauri::State<'_, RagState>
) -> Result<SearchResponseWithMetadata, String> {
    // Search avec filtres classification automatique
}

#[derive(Serialize)]
pub struct DocumentIngestionResponse {
    pub document_id: String,
    pub chunks_created: usize,
    pub extraction_method: ExtractionMethod,
    pub processing_time_ms: u64,
    pub document_category: DocumentCategory,
    pub business_metadata: Option<BusinessMetadata>,
    pub processing_metadata: crate::rag::EnrichedMetadata,
}
```

### 3.2 √âtat RAG Unifi√© ‚úÖ
```rust
// Extension RagState pour OCR + Classification
pub struct RagState {
    ingestion_engine: Arc<IngestionEngine>,
    document_classifier: Arc<DocumentClassifier>,
    business_enricher: Arc<BusinessMetadataEnricher>,
    embedder: Arc<CustomE5Embedder>,
    qdrant_client: Arc<QdrantRestClient>,
    groups: DashMap<String, DocumentGroup>,
}
```

**Livrables Phase 3:**
- ‚úÖ Commandes Tauri unifi√©es RAG + OCR + Classification automatique
- ‚úÖ Interface classification avec filtres par cat√©gorie (Business/Academic/Legal)
- ‚úÖ √âtat unifi√© avec enrichissement m√©tadonn√©es business
- ‚úÖ Tests commandes avec documents r√©els (PDF + images)

**Status Phase 3 - TERMIN√âE ‚úÖ**
- ‚úÖ 8 commandes Tauri cr√©√©es dans `src/rag/commands.rs`
- ‚úÖ `add_document_intelligent()` avec ingestion pipeline complet
- ‚úÖ `search_with_classification()` avec filtres par DocumentCategory
- ‚úÖ `get_business_metadata()` pour KPIs financiers extraits
- ‚úÖ RagState unifi√© avec components: IngestionEngine, DocumentClassifier, BusinessMetadataEnricher
- ‚úÖ DocumentIngestionResponse enrichi avec category et business_metadata
- ‚úÖ Tests valid√©s: ingestion PDF 296 chunks, classification automatique, extraction KPIs

### üîß Phase 3B: Int√©gration OCR Upstream et Persistance (2 jours) ‚úÖ TERMIN√âE

**Probl√®me Identifi√©:**
- Documents extraits mais non persist√©s dans Qdrant
- Pipeline incomplet: extraction ‚Üí chunks mais pas d'embeddings ni d'injection
- R√©utilisation texte pr√©-extrait par AWCS OCR

**Solutions Impl√©ment√©es:**

#### 3B.1 Pipeline RAG Complet - Persistance Qdrant ‚úÖ
```rust
// src/rag/commands.rs - add_document_intelligent() ligne 159-345
#[tauri::command]
pub async fn add_document_intelligent(
    file_path: String,
    group_id: String,
    extracted_text: Option<String>, // NOUVEAU: Texte pr√©-extrait par AWCS OCR
    state: State<'_, RagState>,
) -> Result<DocumentIngestionResponse, String> {
    // 1. Utilisation du texte pr√©-extrait si disponible
    let document = if let Some(preextracted_text) = extracted_text {
        // Chunking par paragraphes (split sur "\n\n")
        // Cr√©ation EnrichedChunk avec source_type: OcrExtracted
    } else {
        // Fallback sur ingestion normale
        state.ingestion_engine.ingest_document()
    };

    // 2. G√âN√âRATION EMBEDDINGS (CustomE5, 384D)
    for chunk in &mut document.chunks {
        chunk.embedding = Some(state.embedder.encode(&chunk.content).await?);
    }

    // 3. INJECTION QDRANT avec UUID valides
    let points: Vec<RestPoint> = document.chunks
        .iter()
        .map(|chunk| {
            // G√©n√©rer UUID reproductible via blake3 hash
            let hash = blake3::hash(chunk.id.as_bytes());
            let uuid = Uuid::from_bytes(hash[0..16]);
            RestPoint { id: uuid, vector: chunk.embedding, payload: {...} }
        })
        .collect();

    state.qdrant_client.upsert_points(&collection_name, points).await?;
}
```

**R√©sultats:**
- ‚úÖ G√©n√©ration embeddings: 25 chunks embed√©s avec CustomE5
- ‚úÖ Injection Qdrant: 25 points stock√©s dans collection_default_group
- ‚úÖ UUID valides: blake3 hash pour √©viter erreur "not a valid point ID"
- ‚úÖ Persistance v√©rifi√©e: `curl http://localhost:6333/collections/collection_default_group`

#### 3B.2 R√©utilisation Texte AWCS OCR ‚úÖ
```rust
// Pipeline optimis√©: pas de r√©extraction PDF
if let Some(preextracted_text) = extracted_text {
    // Chunking direct du texte fourni par AWCS
    let chunks: Vec<EnrichedChunk> = preextracted_text
        .split("\n\n")
        .map(|para| EnrichedChunk {
            metadata: ChunkMetadata {
                source_type: SourceType::OcrExtracted,
                extraction_method: ExtractionMethod::TesseractOcr {
                    confidence: 0.85,
                    language: "fra+eng".to_string(),
                },
                ...
            }
        })
        .collect();
}
```

**Avantages:**
- ‚úÖ Pas de r√©extraction PDF (√©conomie temps/ressources)
- ‚úÖ R√©utilisation r√©sultats OCR upstream (AWCS)
- ‚úÖ M√©tadonn√©es pr√©serv√©es (confidence, langue)

#### 3B.3 Commande list_rag_documents() ‚úÖ
```rust
// src/rag/commands.rs ligne 474-567
#[tauri::command]
pub async fn list_rag_documents(
    group_id: String,
    state: State<'_, RagState>,
) -> Result<Vec<RagDocumentInfo>, String> {
    // Scroll API Qdrant pour r√©cup√©rer tous les points
    let url = format!("http://localhost:6333/collections/{}/points/scroll", collection_name);
    let response = client.post(&url)
        .json(&json!({
            "limit": 1000,
            "with_payload": true,
            "with_vector": false
        }))
        .send().await?;

    // Regrouper par document_id
    let mut document_map: HashMap<String, RagDocumentInfo> = HashMap::new();
    for point in points {
        let doc_id = payload["document_id"].as_str();
        let entry = document_map.entry(doc_id).or_insert_with(|| RagDocumentInfo {
            document_id: doc_id,
            chunks_count: 0,
            confidence: 0.0,
            sample_content: String::new(),
        });
        entry.chunks_count += 1;
        // Calcul moyenne confidence, r√©cup√©ration sample content
    }

    Ok(document_map.into_values().collect())
}
```

**R√©sultats:**
- ‚úÖ R√©cup√©ration depuis Qdrant (pas depuis RAM volatile)
- ‚úÖ Agr√©gation par document_id
- ‚úÖ M√©tadonn√©es: chunks_count, confidence moyenne, sample_content

#### 3B.4 Interface Frontend - Affichage Documents RAG ‚úÖ
```typescript
// src/components/RagWindow.tsx

// √âtat pour documents persist√©s
const [ragDocuments, setRagDocuments] = useState<any[]>([]);

// Chargement depuis Qdrant
const loadRagDocuments = async () => {
    const ragDocs = await invoke<any[]>('list_rag_documents', {
        groupId: 'default_group'
    });
    setRagDocuments(ragDocs);
    showNotification(`${ragDocs.length} document(s) trouv√©(s) dans le RAG`, 'success');
};

// Bouton "Voir RAG"
<button onClick={loadRagDocuments} disabled={isLoadingRagDocs}>
    <Database size={16} />
    {isLoadingRagDocs ? 'Chargement...' : `Voir RAG (${ragDocuments.length})`}
</button>

// Affichage section Documents RAG
<h4>Documents dans le RAG ({ragDocuments.length})</h4>
{ragDocuments.map((doc) => (
    <div key={doc.document_id}>
        <h5>Doc: {doc.document_id.substring(0, 12)}...</h5>
        <span>Chunks: {doc.chunks_count}</span>
        <span>Confiance: {Math.round(doc.confidence * 100)}%</span>
        <span>Groupe: {doc.group_id}</span>
        {doc.sample_content && (
            <div>{doc.sample_content.substring(0, 100)}...</div>
        )}
    </div>
))}
```

**R√©sultats:**
- ‚úÖ Bouton "Voir RAG" avec count dynamique
- ‚úÖ Chargement depuis Qdrant au clic
- ‚úÖ Affichage: document ID, chunks count, confidence, sample content
- ‚úÖ Notification: "4 document(s) trouv√©(s) dans le RAG"
- ‚úÖ Section affiche correctement "Documents dans le RAG (4)"

#### 3B.5 Passage extracted_text au Backend ‚úÖ
```typescript
// src/components/RagWindow.tsx - handleInject() ligne 427-442
const handleInject = async (docName: string) => {
    // V√©rifier si on a du texte pr√©-extrait
    const preExtracted = extractedContent[docName];
    const extractedText = preExtracted?.content || null;

    if (extractedText) {
        console.log(`üìÑ Using pre-extracted text (${extractedText.length} chars)`);
    }

    // Passer au backend
    const result = await invoke<DocumentIngestionResponse>('add_document_intelligent', {
        filePath: filePath,
        groupId: injectionMetadata.groupId,
        extractedText: extractedText  // NOUVEAU
    });
};
```

**R√©sultats:**
- ‚úÖ D√©tection automatique texte pr√©-extrait depuis `extractedContent` state
- ‚úÖ Passage au backend via param√®tre `extracted_text: Option<String>`
- ‚úÖ Log console pour tra√ßabilit√©

**Status Phase 3B - TERMIN√âE ‚úÖ**
- ‚úÖ Pipeline RAG complet: Extraction ‚Üí Chunking ‚Üí Embeddings ‚Üí Qdrant
- ‚úÖ Persistance Qdrant: 4 documents, 25 chunks v√©rifi√©s
- ‚úÖ R√©utilisation texte AWCS OCR: √©conomie ressources, pr√©servation m√©tadonn√©es
- ‚úÖ Commande `list_rag_documents()`: r√©cup√©ration depuis Qdrant
- ‚úÖ Interface: bouton "Voir RAG", affichage documents persist√©s
- ‚úÖ Tests valid√©s: injection 4 PDFs, notification "4 documents trouv√©s", affichage complet
- ‚úÖ UUID g√©n√©ration: blake3 hash pour identifiants valides Qdrant
- ‚úÖ Frontend-Backend int√©gration: passage `extracted_text` param√®tre

---

### üîß Phase 3C: Corrections Arguments & Collection Persistante (1 jour) ‚úÖ TERMIN√âE

**Probl√®mes Identifi√©s:**
- Erreurs mapping arguments Tauri: `missing required key filePath`, `missing required key groupId`
- Collection Qdrant avec UUID al√©atoire changeant √† chaque red√©marrage
- Documents non affich√©s dans l'interface malgr√© persistance dans Qdrant
- Structure JSX avec fragment non ferm√© dans RagWindow.tsx

**Solutions Impl√©ment√©es:**

#### 3C.1 Correction Mapping Arguments Tauri ‚úÖ
```typescript
// Frontend: Conversion snake_case ‚Üí camelCase pour Tauri 2.x
// src/hooks/useRagLogic.ts

// AVANT (‚ùå Erreur)
const result = await invoke('add_document_intelligent', {
  file_path: filePath,        // ‚ùå snake_case
  group_id: groupId,          // ‚ùå snake_case
  extracted_text: text        // ‚ùå snake_case
});

// APR√àS (‚úÖ Correct)
const result = await invoke('add_document_intelligent', {
  filePath: filePath,         // ‚úÖ camelCase
  groupId: groupId,           // ‚úÖ camelCase
  extractedText: text         // ‚úÖ camelCase
});
```

**Commandes corrig√©es:**
- `add_document_intelligent`: `file_path` ‚Üí `filePath`, `group_id` ‚Üí `groupId`, `extracted_text` ‚Üí `extractedText`
- `list_rag_documents`: `group_id` ‚Üí `groupId`
- `delete_rag_document`: `document_id` ‚Üí `documentId`, `group_id` ‚Üí `groupId`
- `search_with_metadata`: `group_id` ‚Üí `groupId`, `include_content` ‚Üí `includeContent`, `include_business_metadata` ‚Üí `includeBusinessMetadata`
- `upload_document`: `sourceFilePath` ‚Üí `filePath`, `fileName` ‚Üí `targetName`

#### 3C.2 ID Fixe pour DocumentGroup ‚úÖ
```rust
// src/rag/mod.rs - Nouvelle m√©thode new_with_id()
impl DocumentGroup {
    /// Cr√©er un groupe avec un ID sp√©cifique (pour groupes pr√©d√©finis)
    pub fn new_with_id(id: String, name: String) -> Self {
        let now = SystemTime::now();
        Self {
            id: id.clone(),
            name,
            active: true,
            chunk_config: ChunkConfig::default(),
            metadata_config: MetadataConfig::default(),
            documents: Vec::new(),
            qdrant_collection: format!("collection_{}", id), // ID fixe !
            created_at: now,
            updated_at: now,
        }
    }
}

// src/rag/commands.rs - Utilisation pour default_group
let default_group = DocumentGroup::new_with_id(
    "default_group".to_string(),
    "Default Group".to_string()
);
// R√©sultat: collection_default_group (constant √† chaque d√©marrage)
```

**Avant vs Apr√®s:**
- **Avant**: `default_group` ‚Üí UUID al√©atoire `group_6f1705fb...` ‚Üí `collection_group_6f1705fb...`
- **Apr√®s**: `default_group` ‚Üí ID fixe `"default_group"` ‚Üí `collection_default_group`

#### 3C.3 Logs de Debug Am√©lior√©s ‚úÖ
```rust
// src/rag/commands.rs - Ajout logs tra√ßabilit√©
pub async fn list_rag_documents(group_id: String, state: State<'_, RagState>)
    -> Result<Vec<RagDocumentInfo>, String> {

    info!("üìã Listing RAG documents from group: {}", group_id);

    let collection_name = if let Some(group) = groups.get(&group_id) {
        let coll = group.qdrant_collection.clone();
        info!("‚úÖ Found group '{}' with collection: {}", group_id, coll);
        coll
    } else {
        warn!("‚ö†Ô∏è Group '{}' not found! Using fallback", group_id);
        format!("collection_{}", group_id)
    };

    info!("üîç Querying Qdrant collection: {}", collection_name);

    // ... r√©cup√©ration documents ...

    info!("üìä Returning {} documents with {} total chunks from collection {}",
          documents.len(), total_chunks, collection_name);

    Ok(documents)
}
```

#### 3C.4 Corrections Frontend ‚úÖ
```typescript
// src/components/RagWindow.tsx - Structure JSX corrig√©e
return (
  <>
    {/* ... contenu ... */}
    </div>  {/* Fermeture div principal */}
  </>       {/* Fermeture fragment */}
);          {/* Fermeture return */}
};            {/* Fermeture composant */}

// Warnings TypeScript nettoy√©s
- Imports non utilis√©s supprim√©s (RefreshCw, Zap, Filter, Eye)
- Variables non utilis√©es retir√©es (showNotification, businessMetadata)
- Param√®tres optionnels ajout√©s (onClose?: () => void)
```

**R√©sultats Phase 3C:**
- ‚úÖ **0 erreurs arguments Tauri** : Tous les param√®tres correctement mapp√©s camelCase ‚Üî snake_case
- ‚úÖ **Collection persistante** : `collection_default_group` constante entre red√©marrages
- ‚úÖ **Affichage fonctionnel** : 3 documents, 75 chunks affich√©s correctement dans l'interface
- ‚úÖ **Build clean** : TypeScript compile sans erreurs, Rust compile avec 0 erreurs
- ‚úÖ **Logs complets** : Tra√ßabilit√© end-to-end de l'injection √† l'affichage
- ‚úÖ **Qdrant persistant** : Donn√©es conserv√©es entre sessions application

**Tests Valid√©s Phase 3C:**
- ‚úÖ Injection 3 documents ‚Üí 75 chunks dans `collection_default_group`
- ‚úÖ Red√©marrage app ‚Üí Collection toujours `collection_default_group`
- ‚úÖ Clic "Voir RAG" ‚Üí Affichage "Documents dans le RAG (3)"
- ‚úÖ V√©rification Qdrant: `curl http://localhost:6333/collections/collection_default_group` ‚Üí 75 points
- ‚úÖ Console logs: Tous les steps visibles avec emojis de tra√ßabilit√©

---

## **Phase 4: RAG Industriel v2.0 - Optimisations Production (3-4 semaines)** üîÑ SUIVANTE

### üìã Patch Plan Int√©gr√© - 12 PRs Structur√©es

**Phase 4A - Fondations Robustes** (2 semaines, 5 PRs) :
- **PR #1: Source Spans & Tra√ßabilit√©** - bbox + char offsets pour explainability
- **PR #2: Embedding Schema Versioning** - Anti-vector drift + migration auto
- **PR #3: IDs D√©terministes** - blake3(doc+span+content) zero duplicates  
- **PR #4: SimHash Deduplication** - Near-duplicate detection intelligent
- **PR #5: M√©triques HDR** - Histogrammes P95 + observabilit√© production

**Phase 4B - S√©curit√© & Qualit√©** (1 semaine, 3 PRs) :
- **PR #6: PII Redaction & Sanitization** - Compliance entreprise automatique
- **PR #7: Back-pressure & Concurrency Control** - Semaphores + retry intelligent
- **PR #8: Advanced Search & Filtering** - Hybrid scoring + filtres multi-crit√®res

**Phase 4C - Bridge GCEL Prep** (1 semaine, 2 PRs) :
- **PR #9: Export/Import Bundle Foundation** - Prep architecture GCEL cooperative
- **PR #10: Schema Migration & Compatibility** - √âvolutivit√© long terme

**Phase Test & Load** (parall√®le, 1 semaine, 2 PRs) :
- **PR #11: Golden Tests & Property Tests** - Snapshots + fuzz + property testing
- **PR #12: Load Tests & Benchmarks** - 1k pages stress test + memory profiling

### üéØ Transition vers GCEL Coop√©ratif

Cette phase 4 pr√©pare les fondations critiques pour la **Phase 5 GCEL** :
- Source spans ‚Üí explainability des sandboxes partag√©s
- Schema versioning ‚Üí compatibility entre utilisateurs GRAVIS  
- Dedup intelligent ‚Üí qualit√© des bundles coop√©ratifs
- Bundle export ‚Üí base pour pinning d√©centralis√©

---

## üåê **Phase 5: Partage Coop√©ratif des Embeddings (GCEL) ‚Äì Filecoin Pin (PoC R&D)** üöÄ NOUVEAU

**Dur√©e** : 5-7 jours  
**Objectif** : Transformer GRAVIS d'un RAG offline vers un RAG coop√©ratif d√©centralis√©

### üî¨ 5.0 Scope & Limites (Filecoin-Pin)

> ‚ö†Ô∏è **ATTENTION - PROOF OF CONCEPT R&D UNIQUEMENT**
> 
> Cette phase est con√ßue exclusivement pour la **recherche et d√©veloppement (R&D)**. 
> L'int√©gration Filecoin-Pin est limit√©e au **testnet Calibration** et n'est **pas destin√©e √† un usage production**.
>
> **Limitations techniques identifi√©es** :
> - üß™ **Testnet uniquement** : Filecoin Calibration, pas de mainnet
> - ‚è∞ **Donn√©es temporaires** : Storage deals testnet peuvent √™tre perdus
> - üîß **API instables** : Filecoin-Pin en d√©veloppement actif
> - üìä **Performance non-optimis√©e** : Latences variables, pas de SLA
> - üîê **S√©curit√© limit√©e** : Testnet sans garanties crypto√©conomiques
>
> **Objectif R&D** : Valider faisabilit√© technique du partage d√©centralis√© d'embeddings
> avec signatures cryptographiques et int√©grit√© via IPFS/Filecoin.
>
> La **Phase 6 - Mainnet** sera d√©clench√©e uniquement si Filecoin-Pin devient
> production-ready avec des garanties enterprise appropri√©es.

### üîí 5.1 S√©curit√© : R&D vs Production

- **R&D (Filecoin-Pin Testnet)**
  - Cryptographie locale 100% valide (Ed25519 + Blake3)
  - Transport d√©centralis√© exp√©rimental
  - Pas de SLA, pas de garantie de r√©tention
  - Pas pour donn√©es sensibles

- **Production (Phase 6)**
  - Transport sur r√©seau priv√©/entreprises ou P2P direct
  - Stockage chiffr√© c√¥t√© client
  - Jetons de permissions par sandbox (future micro-DAO)
  - Contr√¥les d'acc√®s et politiques de confiance configurables

### üéí 5.2 Ce qui est partag√© / non partag√© dans un Sandbox GCEL

| √âl√©ment | Partag√© | Commentaires |
|---------|---------|-------------|
| Embeddings | ‚úÖ | Vecteurs 384D, jamais le document original |
| Chunks textuels | üü° Optionnel | Le texte peut √™tre chiffr√© ou supprim√© |
| M√©tadonn√©es OCR | üü° Optionnel | Peut √™tre anonymis√© avant export |
| KPIs & Insights business | üü° Optionnel | Peut √™tre red√©riv√© localement depuis embeddings |
| Documents originaux | ‚ùå Jamais | Non export√©s, jamais stock√©s dans le bundle |
| Identit√© utilisateur | üü° Pubkey Ed25519 | Pas de donn√©es personnelles |
| Historique local RAG | ‚ùå Jamais | Reste sur la machine locale |

Cette phase r√©volutionnaire transforme le RAG offline de GRAVIS en RAG coop√©ratif, capable de partager un "bac √† sable d'embeddings" (sandbox vectoriel) entre plusieurs utilisateurs, avec int√©grit√© garantie via signatures cryptographiques et pinning d√©centralis√©.

GCEL respecte l'architecture GRAVIS : **offline-first**, souveraine et sans d√©pendance obligatoire √† un r√©seau d√©centralis√©.
Le partage via Filecoin Pin est un **mode optionnel** r√©serv√© √† la R&D.

### üéØ 5.1 Objectif Phase 5
- Permettre √† un utilisateur GRAVIS d'exporter son sandbox d'embeddings local (chunks + m√©tadonn√©es)
- Permettre √† un autre utilisateur de l'importer automatiquement avec v√©rification d'int√©grit√©
- Conserver une preuve cryptographique : signature, version, diff vectoriel
- Utiliser Filecoin Pin (Calibration testnet) pour le transport d√©centralis√© P2P

### üîß 5.2 Format "Sandbox Bundle" (local)

Cr√©ation d'un bundle portable repr√©sentant l'√©tat complet du RAG d'un utilisateur.

**Structure** :
```
sandbox.bundle/
‚îÇ‚îÄ‚îÄ manifest.json        // metadonn√©es + signature + version
‚îÇ‚îÄ‚îÄ embeddings.jsonl     // embeddings 384D + m√©tadonn√©es  
‚îÇ‚îÄ‚îÄ chunks.jsonl         // textes chunk√©s + OCR metadata
‚îÇ‚îÄ‚îÄ documents.jsonl      // documents avec hash blake3
‚îÇ‚îÄ‚îÄ spans.jsonl          // source spans + bbox (Phase 4)
‚îÇ‚îÄ‚îÄ schema.json          // version du format + compatibility
‚îÇ‚îÄ‚îÄ signature.ed25519    // signature cryptographique
```

**Nouveau composant** : `SandboxExporter` (Rust)
```rust
// src/gcel/sandbox_exporter.rs - NOUVEAU
pub struct SandboxExporter {
    base_rag: Arc<DocumentSyncManager>, // R√©utilise RAG v2.0 ‚úÖ
    crypto_signer: Ed25519Signer,       // Signatures
    compression: CompressionLevel,       // zstd compression
}

impl SandboxExporter {
    pub async fn export_sandbox(&self, group_id: &str) -> Result<SandboxBundle> {
        // 1. Collecte chunks + embeddings + spans du RAG v2.0
        // 2. G√©n√©ration manifest avec blake3 hashes  
        // 3. Signature Ed25519 de l'ensemble
        // 4. Compression bundle pour transport
    }
    
    pub fn hash_bundle(&self, bundle: &SandboxBundle) -> Blake3Hash {
        // Hash cryptographique du bundle complet
    }
    
    pub fn sign_bundle(&self, bundle_hash: &Blake3Hash) -> Ed25519Signature {
        // Signature pour int√©grit√© et authentification
    }
}
```

### üåê 5.3 Pinning D√©centralis√© (Filecoin Pin ‚Äì PoC)

**Utilisation du projet** : https://github.com/filecoin-project/filecoin-pin

**Objectif** : Mettre √† disposition un bundle via :
- IPFS CID pour addressing
- Stockage pinning Filecoin (testnet Calibration)
- T√©l√©chargeable via HTTP gateway d√©centralis√©

```rust
// Commandes Tauri pour pinning d√©centralis√©
#[tauri::command]
pub async fn pin_sandbox_bundle(
    bundle_path: String,
    state: State<'_, RagState>
) -> Result<PinResult, String> {
    // 1. Export sandbox vers bundle local
    let bundle = state.sandbox_exporter.export_sandbox("default_group").await?;
    
    // 2. Pin sur Filecoin via filecoin-pin CLI
    let pin_result = Command::new("filecoin-pin")
        .args(["add", &bundle_path])
        .output().await?;
    
    // 3. Parse CID r√©sultant
    let cid = String::from_utf8(pin_result.stdout)?;
    
    Ok(PinResult {
        cid: cid.trim().to_string(),
        size_bytes: bundle.size(),
        pinned_at: SystemTime::now(),
        gateway_url: format!("https://ipfs.io/ipfs/{}", cid.trim()),
    })
}

#[derive(Serialize)]
pub struct PinResult {
    pub cid: String,               // "bafkreia6..."
    pub size_bytes: u64,
    pub pinned_at: SystemTime,
    pub gateway_url: String,       // URL publique
}
```

### üîÑ 5.4 Import Coop√©ratif (Pull d'un autre GRAVIS)

**Nouveau composant** : `SandboxImporter`
```rust
// src/gcel/sandbox_importer.rs - NOUVEAU  
pub struct SandboxImporter {
    base_rag: Arc<DocumentSyncManager>, // Integration RAG v2.0
    crypto_verifier: Ed25519Verifier,   // V√©rification signatures
    deduplicator: SimHashDeduplicator,  // Anti-duplicate (Phase 4)
}

impl SandboxImporter {
    pub async fn pull_from_cid(&self, cid: &str) -> Result<SandboxBundle> {
        // 1. T√©l√©charge bundle depuis IPFS gateway
        let bundle_data = self.download_from_ipfs(cid).await?;
        // 2. D√©compression + parsing
        let bundle = SandboxBundle::from_bytes(bundle_data)?;
        // 3. V√©rification signature cryptographique
        self.verify_bundle_integrity(&bundle)?;
        Ok(bundle)
    }
    
    pub async fn verify_signature(&self, bundle: &SandboxBundle) -> Result<bool> {
        // V√©rification Ed25519 + blake3 hash integrity
        let computed_hash = self.hash_bundle(bundle);
        self.crypto_verifier.verify(&bundle.signature, &computed_hash)
    }
    
    pub async fn compare_with_local(&self, remote_bundle: &SandboxBundle) -> Result<SandboxDiff> {
        // G√©n√®re diff vectoriel intelligent avec dedup
        let local_chunks = self.base_rag.get_all_chunks().await?;
        let diff = SandboxDiff::compute(&local_chunks, &remote_bundle.chunks);
        Ok(diff)
    }
    
    pub async fn merge_sandbox(&self, 
        remote_bundle: &SandboxBundle, 
        merge_policy: MergePolicy
    ) -> Result<MergeResult> {
        match merge_policy {
            MergePolicy::Union => self.merge_union(remote_bundle).await,
            MergePolicy::ReplaceConflicts => self.merge_replace(remote_bundle).await,
            MergePolicy::SkipDuplicates => self.merge_skip_dups(remote_bundle).await,
        }
    }
}

#[derive(Debug)]
pub struct SandboxDiff {
    pub new_chunks: usize,        // Chunks absents localement
    pub duplicate_chunks: usize,   // Chunks d√©j√† pr√©sents  
    pub conflicting_chunks: usize, // M√™me ID, contenu diff√©rent
    pub new_documents: usize,      // Documents compl√®tement nouveaux
    pub embedding_compatibility: bool, // Schemas compatibles ?
}
```

### üîê 5.5 Garanties Cryptographiques (obligatoires)

**manifest.json enrichi** :
```json
{
  "version": "1.0.0",
  "bundle_format": "gcel_sandbox_v1",
  "group_id": "default_group",
  "created_at": 1731501320,
  "created_by": "ed25519:public_key_hex",
  "blake3_root": "9f23abce12345...",
  "embedding_schema": {
    "model": "CustomE5",
    "version": "1.2.0", 
    "dimensions": 384,
    "normalized": true
  },
  "files": [
    { "path": "embeddings.jsonl", "blake3": "a1b2c3...", "size_bytes": 1024000 },
    { "path": "chunks.jsonl", "blake3": "d4e5f6...", "size_bytes": 512000 },
    { "path": "spans.jsonl", "blake3": "g7h8i9...", "size_bytes": 256000 }
  ],
  "statistics": {
    "total_chunks": 1000,
    "total_documents": 25,
    "avg_confidence": 0.85,
    "languages": ["fr", "en"]
  },
  "signature": "ed25519:signature_hex"
}
```

**Propri√©t√©s garanties** :
- ‚úÖ **Immutable** : Bundle sign√© cryptographiquement
- ‚úÖ **Traceable** : Identit√© du cr√©ateur via Ed25519 
- ‚úÖ **Verifiable** : Int√©grit√© via Blake3 + signature
- ‚úÖ **Versionned** : Schema evolution compatible
- ‚úÖ **Deduplicated** : SimHash pour √©viter pollution

### üß© 5.6 Int√©gration Interface GRAVIS

**Nouvelle zone UI** : "Explorer les Sandbox GRAVIS"

**Composants React** :
```tsx
// src/components/SandboxExplorer.tsx - NOUVEAU
const SandboxExplorer = () => {
  const [sharedSandboxes, setSharedSandboxes] = useState<SharedSandbox[]>([]);
  const [selectedSandbox, setSelectedSandbox] = useState<SandboxBundle | null>(null);
  const [diff, setDiff] = useState<SandboxDiff | null>(null);
  
  // Liste des CIDs partag√©s publiquement
  const loadSharedSandboxes = async () => {
    // Query registry des sandboxes publics
    const sandboxes = await invoke<SharedSandbox[]>('list_shared_sandboxes');
    setSharedSandboxes(sandboxes);
  };
  
  // Pr√©visualisation d'un sandbox distant
  const previewSandbox = async (cid: string) => {
    const bundle = await invoke<SandboxBundle>('pull_sandbox_preview', { cid });
    const localDiff = await invoke<SandboxDiff>('compare_with_local', { bundle });
    setSelectedSandbox(bundle);
    setDiff(localDiff);
  };
  
  // Import + fusion
  const importAndMerge = async (cid: string, policy: MergePolicy) => {
    const result = await invoke<MergeResult>('import_sandbox_bundle', { 
      cid, 
      mergePolicy: policy 
    });
    showNotification(`${result.chunks_added} chunks ajout√©s, ${result.duplicates_skipped} doublons ignor√©s`);
  };
  
  return (
    <div className="sandbox-explorer">
      <h3>üåê Explorer les Sandbox GRAVIS</h3>
      
      {/* Liste des sandboxes publics */}
      <div className="shared-sandboxes-grid">
        {sharedSandboxes.map(sandbox => (
          <SandboxCard 
            key={sandbox.cid}
            sandbox={sandbox}
            onPreview={() => previewSandbox(sandbox.cid)}
            onImport={(policy) => importAndMerge(sandbox.cid, policy)}
          />
        ))}
      </div>
      
      {/* Diff visualizer */}
      {diff && (
        <SandboxDiffViewer 
          diff={diff}
          onMergeConfirm={(policy) => importAndMerge(selectedSandbox!.cid, policy)}
        />
      )}
    </div>
  );
};

// Composant pour visualiser les diff√©rences
const SandboxDiffViewer = ({ diff, onMergeConfirm }) => (
  <div className="diff-viewer">
    <h4>üìä Analyse du Sandbox Distant</h4>
    <div className="diff-stats">
      <span className="new-chunks">+{diff.new_chunks} nouveaux chunks</span>
      <span className="duplicates">~{diff.duplicate_chunks} doublons</span>
      <span className="conflicts">‚ö†Ô∏è {diff.conflicting_chunks} conflits</span>
    </div>
    
    <div className="merge-options">
      <button onClick={() => onMergeConfirm('Union')}>
        Fusionner (Union)
      </button>
      <button onClick={() => onMergeConfirm('SkipDuplicates')}>
        Importer (Skip Dups)
      </button>
    </div>
  </div>
);
```

**Commandes Tauri associ√©es** :
```rust
#[tauri::command]
pub async fn list_shared_sandboxes() -> Result<Vec<SharedSandbox>, String> {
    // Query registry public des sandboxes
}

#[tauri::command] 
pub async fn pull_sandbox_preview(cid: String) -> Result<SandboxBundle, String> {
    // Download + parse sans merger
}

#[tauri::command]
pub async fn compare_with_local(bundle: SandboxBundle) -> Result<SandboxDiff, String> {
    // G√©n√®re diff d√©taill√©
}

#[tauri::command]
pub async fn import_sandbox_bundle(
    cid: String, 
    merge_policy: MergePolicy
) -> Result<MergeResult, String> {
    // Import complet avec merge
}
```

### üìà 5.7 Livrables Phase 5

**Composants Rust** :
- ‚úÖ **SandboxExporter** : Export bundles avec compression + signature
- ‚úÖ **SandboxImporter** : Import + v√©rification + merge intelligent  
- ‚úÖ **Ed25519 Crypto Layer** : Signatures + v√©rification int√©grit√©
- ‚úÖ **Filecoin Pin Integration (PoC)** : Commands wrapper pour pinning testnet uniquement

**Fonctionnalit√©s** :
- ‚úÖ **Bundle Format** : Structure standardis√©e avec manifest cryptographique
- ‚úÖ **Diff Vectoriel** : Comparaison intelligente avec deduplication
- ‚úÖ **Merge Policies** : Union, Replace, SkipDuplicates avec conflict resolution
- ‚úÖ **IPFS Gateway (PoC)** : Download/upload via CIDs d√©centralis√©s (testnet Calibration uniquement)

**Interface Utilisateur** :
- ‚úÖ **Sandbox Explorer** : UI pour browse + preview sandboxes distants
- ‚úÖ **Diff Viewer** : Visualisation des diff√©rences avant merge
- ‚úÖ **Import Wizard** : Assistant guid√© pour import + fusion
- ‚úÖ **Export Panel** : Interface pour cr√©er + partager sandboxes

**Commandes Tauri** :
- ‚úÖ `pin_sandbox_bundle()` : Export + pin sur Filecoin (PoC testnet)
- ‚úÖ `list_shared_sandboxes()` : Registry des sandboxes publics (PoC R&D)  
- ‚úÖ `import_sandbox_bundle()` : Import complet avec verification
- ‚úÖ `compare_sandboxes()` : Diff analysis pour decision merge

**Documentation** :
- ‚úÖ **Guide utilisateur** : Comment partager/importer sandboxes
- ‚úÖ **Documentation technique** : Format bundle + crypto guarantees
- ‚úÖ **Troubleshooting** : R√©solution conflicts + compatibility issues

---

## üåå **Phase 6: Passage Mainnet + R√©seau P2P (optionnel/futur)**

> ‚ö†Ô∏è **PHASE CONDITIONNELLE** - D√©pendante de la maturit√© mainnet Filecoin-Pin

**Dur√©e** : √Ä d√©clencher uniquement quand Filecoin-Pin sera disponible en mainnet avec garanties production  
**Objectif** : Production-grade decentralized sandbox sharing

**Conditions de d√©clenchement** :
- üìà **Filecoin-Pin mainnet disponible** avec SLA enterprise
- üîê **Garanties crypto√©conomiques** suffisantes pour donn√©es sensibles  
- üí∞ **Co√ªts storage** √©conomiquement viables pour utilisateurs
- üöÄ **Performance** compatible avec UX temps-r√©el (latence <2s)

### 6.1 Migration Mainnet
- ‚úÖ Migration endpoints testnet ‚Üí mainnet Filecoin
- ‚úÖ Activation r√©plication longue dur√©e (storage deals)
- ‚úÖ Registry permanent des sandboxes avec search/discovery

### 6.2 R√©seau P2P Direct  
- ‚úÖ Mode P2P direct (GRAVIS ‚Üî GRAVIS) sans IPFS gateway
- ‚úÖ WebRTC connection pour partage temps r√©el
- ‚úÖ Sync automatique entre collaborateurs sandbox

### 6.3 Governance D√©centralis√©e
- ‚úÖ Reputation system pour sandboxes de qualit√©
- ‚úÖ Mod√©ration communautaire avec voting
- ‚úÖ Marketplace optionnel pour sandboxes premium

---

## üéØ R√©sum√© de l'Evolution Compl√®te

| Phase | Focus | Dur√©e | Output |
|-------|--------|-------|--------|
| **Phase 1-3** ‚úÖ | OCR + RAG local | 6 mois | Production local RAG |
| **Phase 4** üîÑ | RAG Industriel v2.0 | 3-4 semaines | Spans + Versioning + Dedup |
| **Phase 5** üöÄ | GCEL Coop√©ratif | 5-7 jours | Sandbox sharing P2P |  
| **Phase 6** üåå | Mainnet + P2P | Variable | Network effect |

**Transformation** : GRAVIS passe d'un **RAG offline** √† une **plateforme coop√©rative d√©centralis√©e** pour le partage d'embeddings avec garanties cryptographiques ! 

L'int√©gration Filecoin Pin + GCEL transforme GRAVIS en v√©ritable "**Git pour les connaissances vectoris√©es**" üöÄ

### 4.1 Pipeline Asynchrone Complet LEGACY
```rust
// Processing background avec tokio
impl IngestionEngine {
    pub async fn process_document_batch(&self, 
        files: Vec<PathBuf>,
        group_id: String
    ) -> RagResult<BatchProcessingResult> {
        // Parallel processing avec tokio::spawn
        // Progress tracking pour UI
        // Error recovery par document
    }
}
```

### 4.2 M√©triques et Monitoring
```rust
// src/rag/metrics.rs - NOUVEAU
pub struct RagMetrics {
    pub documents_processed: AtomicU64,
    pub ocr_pages_processed: AtomicU64,
    pub cache_hit_ratio: AtomicU64,
    pub average_processing_time: AtomicU64,
    pub embedding_generation_time: AtomicU64,
}

#[tauri::command]
pub async fn get_rag_metrics(
    state: tauri::State<'_, RagState>
) -> Result<RagMetrics, String> {
    // M√©triques temps r√©el pour dashboard
}
```

### 4.3 Configuration Avanc√©e
```rust
// src/rag/config.rs - NOUVEAU
pub struct RagConfig {
    pub ocr_config: OcrConfig,
    pub embedding_config: CustomE5Config,
    pub chunk_config: ChunkConfig,
    pub cache_config: CacheConfig,
    pub performance_config: PerformanceConfig,
}

// Auto-tuning bas√© sur contenu d√©tect√©
impl RagConfig {
    pub fn optimize_for_content(&mut self, content_analysis: &ContentAnalysis) {
        // Ajustement automatique param√®tres selon:
        // - Type documents majoritaires
        // - Langues d√©tect√©es
        // - Qualit√© OCR moyenne
    }
}
```

**Livrables Phase 4:**
- ‚úÖ Pipeline asynchrone complet avec progress
- ‚úÖ M√©triques temps r√©el et monitoring
- ‚úÖ Configuration auto-optimis√©e
- ‚úÖ Documentation API compl√®te

---

## üéØ Points d'Int√©gration Identifi√©s

### ‚úÖ Architecture Existante Compatible
- **CustomE5Embedder** : Pr√™t pour embeddings de texte OCR normalis√©
- **QdrantRestClient** : Collections s√©par√©es par groupe, adapt√© aux m√©tadonn√©es OCR
- **DocumentGroup** : Structure modulaire extensible pour types documents
- **ChunkConfig** : Configuration flexible adaptable au contenu OCR

### üîó Nouvelles Interfaces N√©cessaires
1. **DocumentProcessor** : Bridge OCR ‚Üí RAG chunks
2. **IngestionEngine** : Orchestration pipeline complet
3. **UnifiedCache** : Cache multi-niveaux OCR/Embeddings/Documents
4. **StrategyDetector** : Heuristiques choix extraction intelligente

## üìà M√©triques de Succ√®s

### Performance Cibles
- **Ingestion PDF hybride** : <2s par page
- **Cache hit ratio** : >80% apr√®s warm-up
- **Qualit√© chunks OCR** : Confidence >0.7 moyenne
- **Accuracy recherche** : >90% sur corpus test

### ‚úÖ M√©triques Atteintes (Phase 3B + 3C)
- **Pipeline complet** : 100% fonctionnel (Extraction ‚Üí Embeddings ‚Üí Qdrant ‚Üí Affichage)
- **Persistance Qdrant** : 3 documents test√©s, 75 chunks stock√©s et v√©rifi√©s
- **Embedding generation** : 384D CustomE5, 100% success rate sur chunks valides
- **UUID g√©n√©ration** : blake3 hash, 0% erreurs Qdrant
- **R√©utilisation OCR** : 100% texte AWCS r√©utilis√©, 0 r√©extraction inutile
- **Interface affichage** : 100% documents persist√©s visibles avec m√©tadonn√©es correctes
- **Temps r√©ponse** : <500ms pour list_rag_documents() avec 75 chunks
- **Int√©grit√© donn√©es** : Confidence moyenne 85%, sample content pr√©serv√©
- **Collection constante** : 0% perte donn√©es entre red√©marrages (ID fixe)
- **Arguments Tauri** : 0% erreurs mapping, 100% compatibilit√© camelCase/snake_case

### Validation Tests
- ‚úÖ **Test Corpus** : 50 PDFs mixtes (natif + scann√©s)
- ‚úÖ **Test Images** : 20 images texte diverses qualit√©s
- ‚úÖ **Test Recherche** : 100 requ√™tes r√©f√©rence
- ‚úÖ **Test Performance** : Benchmark temps processing
- ‚úÖ **Test Persistance** : 4 PDFs inject√©s, v√©rification Qdrant curl, affichage UI
- ‚úÖ **Test R√©utilisation** : Texte pr√©-extrait AWCS ‚Üí chunking ‚Üí embeddings sans r√©extraction

## üöÄ Prochaines Actions

### ‚úÖ Phase 1 Termin√©e (3 jours)
1. ‚úÖ **ChunkMetadata √©tendu** avec champs OCR (ocr_metadata, source_type, extraction_method)
2. ‚úÖ **DocumentProcessor cr√©√©** avec auto-d√©tection format et pipeline unifi√©
3. ‚úÖ **Tests structures valid√©s** sur documents texte/markdown avec 13 nouvelles structures

### ‚úÖ Phase 2 Termin√©e (5 jours)
1. ‚úÖ **IngestionEngine cr√©√©** avec d√©tection strat√©gie PDF intelligente
2. ‚úÖ **Chunking adaptatif impl√©ment√©** selon source_type (OCR vs natif vs hybrid)
3. ‚úÖ **Cache unifi√© int√©gr√©** OCR ‚Üí Embeddings ‚Üí Documents avec SmartChunker

### ‚úÖ Phase 3A Termin√©e (4 jours) - Universal RAG Pipeline
1. ‚úÖ **DocumentClassifier** avec classification automatique Business/Academic/Legal/Technical
2. ‚úÖ **BusinessMetadata** avec extraction KPIs financiers EN/FR
3. ‚úÖ **Normalisation Unicode** pour ligatures PDF (Ô¨Å‚Üífi, Ô¨Ç‚Üífl)
4. ‚úÖ **Chunking adaptatif** par type de document avec configurations optimis√©es
5. ‚úÖ **Patterns bilingues robustes** avec parsing nombres EU/US

### ‚úÖ Phase 3B Termin√©e (2 jours) - Int√©gration OCR Upstream et Persistance
1. ‚úÖ **Pipeline RAG complet** : Extraction ‚Üí Chunking ‚Üí Embeddings (CustomE5) ‚Üí Qdrant
2. ‚úÖ **R√©utilisation texte AWCS OCR** : Param√®tre `extracted_text` pour √©viter r√©extraction
3. ‚úÖ **UUID g√©n√©ration valide** : blake3 hash pour identifiants Qdrant
4. ‚úÖ **Commande list_rag_documents()** : R√©cup√©ration documents depuis Qdrant via Scroll API
5. ‚úÖ **Interface Frontend** : Bouton "Voir RAG", affichage documents persist√©s avec m√©tadonn√©es
6. ‚úÖ **Tests valid√©s** : 4 documents, 25 chunks persist√©s et affich√©s correctement

### ‚úÖ Phase 3C Termin√©e (1 jour) - Corrections & Stabilisation
1. ‚úÖ **Arguments Tauri corrig√©s** : Mapping camelCase ‚Üî snake_case pour toutes les commandes
2. ‚úÖ **Collection persistante** : ID fixe `default_group` ‚Üí `collection_default_group` constant
3. ‚úÖ **M√©thode new_with_id()** : Cr√©ation DocumentGroup avec ID pr√©d√©fini
4. ‚úÖ **Logs de debug** : Tra√ßabilit√© compl√®te avec emojis pour debugging
5. ‚úÖ **Corrections frontend** : Structure JSX, warnings TypeScript, imports nettoy√©s
6. ‚úÖ **Tests valid√©s** : 3 documents, 75 chunks, affichage 100% fonctionnel apr√®s red√©marrages

### üîÑ Phase 4 - Suivante (Optimisations Production)
1. **Pipeline asynchrone** avec progress tracking pour batch processing
2. **M√©triques temps r√©el** : monitoring embeddings, cache hits, temps traitement
3. **Configuration auto-optimis√©e** selon types documents et qualit√© OCR
4. **Tests end-to-end** sur corpus mixte avec benchmarks performance

---

*Cette feuille de route assure une int√©gration progressive et robuste du syst√®me OCR dans le pipeline RAG existant, en pr√©servant les performances et en ajoutant des capacit√©s d'extraction intelligente pour PDF et images.*