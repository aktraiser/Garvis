// Test Phase 2 simplifiÃ© - PDF OCR + Injection
// Test avec DeepSeek-OCR PDF et pipeline Phase 2

use std::path::Path;

use gravis_app_lib::rag::{
    IngestionEngine, DocumentProcessor, UnifiedCache, StrategyDetector,
    ChunkConfig, ChunkStrategy, SourceType
};
use gravis_app_lib::rag::ocr::{
    TesseractProcessor, TesseractConfig, OcrCache, CacheConfig
};
use gravis_app_lib::rag::custom_e5::{CustomE5Embedder, CustomE5Config};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    tracing_subscriber::fmt::init();
    
    println!("ğŸš€ Test Phase 2 - PDF DeepSeek-OCR avec Pipeline Intelligent");
    
    // 1. VÃ©rifier prÃ©sence PDF
    let pdf_path = Path::new("../2510.18234v1.pdf");
    if !pdf_path.exists() {
        eprintln!("âŒ PDF non trouvÃ©: {:?}", pdf_path);
        eprintln!("   Placez 2510.18234v1.pdf dans gravis-app/");
        return Ok(());
    }
    
    let file_size = tokio::fs::metadata(pdf_path).await?.len();
    println!("âœ… PDF trouvÃ©: {:.1}MB", file_size as f64 / 1_000_000.0);
    
    // 2. Setup composants Phase 2
    println!("\nğŸ”§ Setup pipeline Phase 2...");
    
    let tesseract_config = TesseractConfig::default();
    let ocr_processor = TesseractProcessor::new(tesseract_config).await?;
    
    let e5_config = CustomE5Config::default();
    let embedder = CustomE5Embedder::new(e5_config).await?;
    
    let document_processor = DocumentProcessor::new(ocr_processor, embedder).await?;
    let unified_cache = {
        let cache_config = CacheConfig::default();
        let ocr_cache = OcrCache::new(cache_config).await?;
        UnifiedCache::new(ocr_cache)
    };
    
    let ingestion_engine = IngestionEngine::new(document_processor);
    let strategy_detector = StrategyDetector::new();
    
    println!("  âœ“ Pipeline initialisÃ©");
    
    // 3. Test dÃ©tection stratÃ©gie PDF
    println!("\nğŸ” Test dÃ©tection stratÃ©gie PDF...");
    let strategy = strategy_detector.detect_strategy(pdf_path).await?;
    println!("  âœ“ StratÃ©gie dÃ©tectÃ©e: {:?}", strategy);
    
    // 4. Test extraction et chunking avec IngestionEngine
    println!("\nğŸ“„ Test extraction PDF avec chunking intelligent...");
    
    let chunk_config = ChunkConfig {
        chunk_size: 100,  // Chunks adaptÃ©s pour test
        overlap: 20,
        strategy: ChunkStrategy::Heuristic,
    };
    
    let start_extraction = std::time::Instant::now();
    let ingestion_result = ingestion_engine
        .ingest_document(pdf_path, "deepseek_test_group", &chunk_config)
        .await?;
    let extraction_time = start_extraction.elapsed();
    
    println!("  âœ… Extraction terminÃ©e en {:?}", extraction_time);
    println!("  âœ“ Document ID: {}", ingestion_result.document.id);
    println!("  âœ“ Chunks crÃ©Ã©s: {}", ingestion_result.document.chunks.len());
    println!("  âœ“ Contenu total: {} caractÃ¨res", ingestion_result.document.content.len());
    println!("  âœ“ Temps traitement: {}ms", ingestion_result.processing_time_ms);
    
    // 5. Analyse qualitÃ© chunking par type
    println!("\nâœ‚ï¸  Analyse qualitÃ© chunking...");
    
    let mut ocr_chunks = 0;
    let mut native_chunks = 0;
    let mut high_confidence_chunks = 0;
    let mut total_confidence = 0.0;
    
    for chunk in &ingestion_result.document.chunks {
        total_confidence += chunk.metadata.confidence;
        
        match chunk.metadata.source_type {
            SourceType::OcrExtracted => ocr_chunks += 1,
            SourceType::NativeText => native_chunks += 1,
            _ => {}
        }
        
        if chunk.metadata.confidence >= 0.8 {
            high_confidence_chunks += 1;
        }
    }
    
    let avg_confidence = total_confidence / ingestion_result.document.chunks.len() as f32;
    
    println!("  âœ“ Chunks OCR: {}", ocr_chunks);
    println!("  âœ“ Chunks natifs: {}", native_chunks);
    println!("  âœ“ Confidence moyenne: {:.2}", avg_confidence);
    println!("  âœ“ Chunks haute confiance (â‰¥0.8): {}", high_confidence_chunks);
    
    // 6. Ã‰chantillon de chunks avec dÃ©tails
    println!("\nğŸ“‹ Ã‰chantillon chunks (5 premiers):");
    
    for (i, chunk) in ingestion_result.document.chunks.iter().take(5).enumerate() {
        let preview = if chunk.content.len() > 60 {
            format!("{}...", &chunk.content[..60])
        } else {
            chunk.content.clone()
        };
        
        println!("  {}. Type={:?}, Conf={:.2}, Taille={}", 
                 i + 1, 
                 chunk.metadata.source_type, 
                 chunk.metadata.confidence,
                 chunk.content.len());
        println!("     \"{}\"", preview);
    }
    
    // 7. Test cache et mÃ©triques
    println!("\nğŸ’¾ Test cache unifiÃ©...");
    
    // Simuler cache embeddings
    for (i, chunk) in ingestion_result.document.chunks.iter().take(3).enumerate() {
        let mock_embedding = vec![0.1; 384]; // Mock embedding 384D
        unified_cache.cache_embedding(&chunk.hash, mock_embedding);
        
        if i == 0 {
            println!("  âœ“ Embedding cached pour chunk: {}", chunk.id);
        }
    }
    
    let cache_metrics = unified_cache.get_cache_metrics();
    println!("  âœ“ Cache embeddings: {} entrÃ©es", cache_metrics.embedding_cache_size);
    println!("  âœ“ MÃ©moire estimÃ©e: {}KB", cache_metrics.memory_usage_estimate / 1024);
    
    // 8. Test nettoyage
    let cleanup_result = unified_cache.cleanup_cache(1000, 24).await?;
    println!("  âœ“ Nettoyage: {} docs, {} embeddings supprimÃ©s", 
             cleanup_result.removed_documents, cleanup_result.removed_embeddings);
    
    // 9. RÃ©sumÃ© final
    println!("\nğŸ“Š RÃ©sumÃ© test Phase 2:");
    println!("  ğŸ“„ PDF: {:.1}MB traitÃ© en {:?}", 
             file_size as f64 / 1_000_000.0, extraction_time);
    println!("  ğŸ§  Pipeline: {} chunks gÃ©nÃ©rÃ©s", ingestion_result.document.chunks.len());
    println!("  ğŸ¯ QualitÃ©: {:.1}% chunks haute confiance", 
             high_confidence_chunks as f64 / ingestion_result.document.chunks.len() as f64 * 100.0);
    println!("  âš¡ Performance: {}ms/chunk", 
             ingestion_result.processing_time_ms as f64 / ingestion_result.document.chunks.len() as f64);
    
    // 10. Test recherche simulÃ©e dans les chunks
    println!("\nğŸ” Test recherche simulÃ©e...");
    
    let search_terms = vec!["deep learning", "optical character", "neural network"];
    
    for term in search_terms {
        let mut matches = 0;
        for chunk in &ingestion_result.document.chunks {
            if chunk.content.to_lowercase().contains(&term.to_lowercase()) {
                matches += 1;
            }
        }
        println!("  \"{}\" â†’ {} chunks contiennent le terme", term, matches);
    }
    
    println!("\nâœ… Test Phase 2 PDF terminÃ© avec succÃ¨s!");
    println!("   Pipeline OCR â†’ Chunking â†’ Cache validÃ© sur document rÃ©el");
    
    Ok(())
}