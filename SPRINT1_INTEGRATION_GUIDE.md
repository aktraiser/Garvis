# Sprint 1 - Niveau 1: LLM Synthesis Integration Guide

> **Date** : 20 novembre 2024
> **Status** : ‚úÖ Backend impl√©ment√©, Frontend pr√™t
> **Next Step** : Int√©gration dans DirectChatPage

---

## üéØ Ce qui a √©t√© impl√©ment√©

### ‚úÖ Backend Rust

**Fichier** : [`gravis-app/src-tauri/src/rag/direct_chat_commands.rs`](gravis-app/src-tauri/src/rag/direct_chat_commands.rs)

1. **Structs ajout√©es** (lignes 99-123) :
   ```rust
   pub struct LlmContextResponse
   pub struct LlmChunkInfo
   ```

2. **Nouvelle commande Tauri** (lignes 289-338) :
   ```rust
   #[tauri::command]
   pub async fn chat_with_llm_context(...)
   ```

3. **Helper function** (lignes 1685-1747) :
   ```rust
   fn build_llm_context(...) -> (String, Vec<LlmChunkInfo>, bool)
   ```

**Features** :
- ‚úÖ Troncature √† 800 chars par chunk (roadmap recommendation)
- ‚úÖ D√©tection automatique des donn√©es OCR
- ‚úÖ Formatting source labels (Figure OCR, Table, Document Text, etc.)
- ‚úÖ Confidence bas√©e sur score top-1 chunk

### ‚úÖ Frontend TypeScript

**Fichier** : [`gravis-app/src/lib/llm-synthesis.ts`](gravis-app/src/lib/llm-synthesis.ts)

1. **Types TypeScript** (lignes 7-34)
2. **Prompt Template** (ligne 36-50) - conforme au roadmap
3. **Fonction principale** :
   ```typescript
   export async function chatWithLlmSynthesis(
     sessionId: string,
     query: string,
     selection?: any | null,
     limit?: number | null
   ): Promise<LlmChatResponse>
   ```

**Flow complet** :
```
User Query
   ‚Üì
chatWithLlmSynthesis()
   ‚Üì
1. invoke("chat_with_llm_context") ‚Üí Rust RAG
   ‚Üì
2. LiteLLMClient.chat() ‚Üí LLM synthesis
   ‚Üì
3. Ajoute OCR warning si n√©cessaire
   ‚Üì
LlmChatResponse (answer + sources + confidence)
```

---

## üìò Comment l'utiliser dans DirectChatPage

### Option A : Remplacement simple (recommand√© pour test)

**Fichier** : `gravis-app/src/hooks/useDirectChat.ts` ou directement dans `DirectChatPage.tsx`

```typescript
import { chatWithLlmSynthesis } from "@/lib/llm-synthesis";

// Dans votre fonction de chat existante, remplacer:
// const response = await invoke("chat_with_dropped_document", { request });

// Par:
const llmResponse = await chatWithLlmSynthesis(
  sessionId,
  userQuery,
  null,  // selection
  10     // limit
);

// Utiliser llmResponse.answer au lieu de response.response
console.log("LLM Answer:", llmResponse.answer);
console.log("Sources:", llmResponse.sources);
console.log("Confidence:", llmResponse.confidence);
```

### Option B : Toggle LLM ON/OFF (production)

```typescript
const [useLlmSynthesis, setUseLlmSynthesis] = useState(true);

const handleChat = async (query: string) => {
  if (useLlmSynthesis) {
    // Niveau 1: LLM Synthesis
    const llmResponse = await chatWithLlmSynthesis(sessionId, query);
    setMessages([...messages, {
      role: "assistant",
      content: llmResponse.answer,
      sources: llmResponse.sources,
      confidence: llmResponse.confidence,
    }]);
  } else {
    // Legacy: Chunks bruts
    const response = await invoke("chat_with_dropped_document", { request });
    setMessages([...messages, {
      role: "assistant",
      content: response.response,
    }]);
  }
};
```

### Option C : Comparaison A/B c√¥te-√†-c√¥te

```typescript
const [showComparison, setShowComparison] = useState(false);

const handleChatWithComparison = async (query: string) => {
  // Appeler les deux en parall√®le
  const [llmResponse, legacyResponse] = await Promise.all([
    chatWithLlmSynthesis(sessionId, query),
    invoke("chat_with_dropped_document", { request }),
  ]);

  setMessages([...messages, {
    role: "assistant",
    llm_answer: llmResponse.answer,
    legacy_answer: legacyResponse.response,
    show_comparison: showComparison,
  }]);
};
```

---

## üß™ Tests Recommand√©s

### Test 1 : Query textuelle simple
```typescript
const query = "What is DeepSeek-OCR?";
const response = await chatWithLlmSynthesis(sessionId, query);

// V√©rifier:
// - response.answer contient "DeepSeek"
// - response.sources.length > 0
// - response.confidence > 0.5
```

### Test 2 : Query avec donn√©es num√©riques
```typescript
const query = "Quelle est la pr√©cision √† compression < 10x ?";
const response = await chatWithLlmSynthesis(sessionId, query);

// V√©rifier:
// - response.answer contient des chiffres (ex: "96.5%", "6.7√ó")
// - response.sources contient des chunks FigureRegionText ou Table
// - response.has_ocr_warning === true
```

### Test 3 : Query sans r√©ponse
```typescript
const query = "What is the meaning of life?";
const response = await chatWithLlmSynthesis(sessionId, query);

// V√©rifier:
// - response.answer contient "ne contient pas" ou "not found"
// - response.confidence est bas
```

---

## üìä Logs √† surveiller

### Backend Rust
```
ü§ñ LLM Context Chat - session: xxx, query: 'your query'
üîç Hybrid search in N chunks for session xxx
‚úÖ Built LLM context from 10 chunks in 150ms (OCR: true)
```

### Frontend TypeScript
```
üîç Fetching LLM context from Rust...
‚úÖ Got context: 10 chunks, 8000 chars
ü§ñ Calling LLM with 9500 chars prompt (10 chunks √ó ~800 chars/chunk)
‚úÖ LLM response: 450 chars, 1200 tokens
‚úÖ LLM Synthesis complete: 2500ms total (RAG: 150ms, LLM: 2350ms)
```

---

## ‚öôÔ∏è Configuration LLM

Assurez-vous que l'utilisateur a configur√© un mod√®le dans le Model Selector :

1. Ouvrir le Model Selector (`cmd+shift+M` ou menu)
2. S√©lectionner un mod√®le (ex: GPT-4o-mini, Claude 3.5 Haiku, Ollama local)
3. Configurer la connexion (LiteLLM, Modal, Ollama, etc.)

Le syst√®me utilise automatiquement `modelConfigStore.getConfig()` pour obtenir la config actuelle.

---

## üêõ Troubleshooting

### Erreur : "LLM synthesis failed"
**Cause** : LLM non configur√© ou API key invalide
**Solution** : V√©rifier Model Selector et connexion active

### Erreur : "Search failed"
**Cause** : Session ID invalide ou expir√©e (TTL 2h)
**Solution** : Recharger le document ou v√©rifier que la session existe

### Latence trop √©lev√©e (> 5s)
**Cause** : Mod√®le trop gros ou trop de chunks
**Solution** :
- Utiliser mod√®le plus petit (Haiku, GPT-4o-mini)
- R√©duire `limit` de 10 √† 5 chunks
- Activer streaming (TODO Niveau 2)

### R√©ponses incoh√©rentes
**Cause** : Contexte trop fragment√© ou chunks non pertinents
**Solution** :
- V√©rifier logs RAG (scores des chunks)
- Consid√©rer Niveau 2 (Query Rewriting) si queries en FR
- Ajuster prompt template si n√©cessaire

---

## üöÄ Next Steps

### Sprint 2 : Niveau 2 - Query Rewriting
- [ ] D√©tecter queries fran√ßaises
- [ ] Call LLM pour rewriting FR ‚Üí EN
- [ ] Cache des rewrites
- [ ] Mesurer am√©lioration recall

### Sprint 3 : Niveau 3 - LLM Reranking (optionnel)
- [ ] D√©cider GO/NO-GO bas√© sur m√©triques N1+N2
- [ ] Impl√©menter si gap > 5%

---

## üìù Checklist Int√©gration

- [ ] Importer `chatWithLlmSynthesis` dans DirectChatPage
- [ ] Tester avec 3 queries vari√©es
- [ ] V√©rifier affichage sources
- [ ] V√©rifier avertissement OCR
- [ ] Mesurer latence (target < 3s P95)
- [ ] Collecter feedback utilisateur
- [ ] Logger m√©triques (search_time_ms, llm_time_ms, confidence)

---

**Auteur** : Claude (Assistant IA Anthropic)
**Version** : 1.0 - Sprint 1 Integration Guide
**Status** : ‚úÖ Ready for Integration
